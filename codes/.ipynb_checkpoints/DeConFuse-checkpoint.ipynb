{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import os \n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.pooling import *\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from torch.autograd import Variable\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from data_processing import * \n",
    "import scipy as sp \n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calOutShape(input_shape,ksize1=3,ksize2 =3,stride=1,maxpool1=False, maxpool2=False, mpl_ksize=2):\n",
    "    mpl_stride = 2\n",
    "    pad = ksize1//2\n",
    "    dim2 = int((input_shape[2]-ksize1+2*pad)/stride) + 1\n",
    "    if maxpool1 == True:\n",
    "        dim2 = (dim2 - mpl_ksize)//mpl_stride + 1\n",
    "    pad = ksize2//2\n",
    "    dim2 = int((dim2-ksize2+2*pad)/stride) + 1\n",
    "    if maxpool2 == True:\n",
    "        dim2 = (dim2 - mpl_ksize)//mpl_stride + 1\n",
    "    return dim2\n",
    "\n",
    "\n",
    "\n",
    "class Transform(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_shape, out_planes1 = 8, out_planes2 = 16,ksize1 = 3,ksize2 = 3,maxpool1 = False, \n",
    "                 maxpool2 = False,mpl_ksize=2):\n",
    "        super(Transform, self).__init__()\n",
    "        self.ksize1 = ksize1\n",
    "        self.ksize2 = ksize2\n",
    "        self.mpl_ksize = mpl_ksize\n",
    "        self.out_planes1 = out_planes1\n",
    "        self.out_planes2 = out_planes2\n",
    "        self.init_T()\n",
    "        self.maxpool1 = maxpool1\n",
    "        self.maxpool2 = maxpool2\n",
    "        self.input_shape = input_shape\n",
    "        self.i = 1\n",
    "        self.atom_ratio = 0.5\n",
    "        self.init_X()\n",
    "        self.gap = AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def init_T(self):\n",
    "        conv = nn.Conv1d(1, out_channels = self.out_planes1, kernel_size = self.ksize1, stride=1, bias=True)\n",
    "        self.T1 = conv._parameters['weight']\n",
    "        conv = nn.Conv1d(in_channels = self.out_planes1, out_channels = self.out_planes2, \n",
    "                         kernel_size = self.ksize2, stride=1, bias=True)\n",
    "        self.T2 = conv._parameters['weight']\n",
    "\n",
    "       \n",
    "           \n",
    "        \n",
    "    def init_X(self):\n",
    "        dim2 = calOutShape(self.input_shape,self.ksize1,self.ksize2,stride = 1,maxpool1 = self.maxpool1, \n",
    "                           maxpool2 = self.maxpool2, mpl_ksize = self.mpl_ksize)\n",
    "        X_shape = [self.input_shape[0],self.out_planes2,dim2]\n",
    "        self.X  = nn.Parameter(torch.randn(X_shape), requires_grad=True)\n",
    "        self.num_features = self.out_planes2*dim2\n",
    "        self.num_atoms = int(self.num_features*self.atom_ratio*5) #dim2//2\n",
    "        T_shape = [self.num_atoms,self.num_features]\n",
    "        self.T = nn.Parameter(torch.randn(T_shape), requires_grad=True)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = F.conv1d(inputs, self.T1,padding = self.ksize1//2)\n",
    "        if self.maxpool1:\n",
    "            x = F.max_pool1d(x, 2)\n",
    "        x = F.selu(x)\n",
    "        x = F.conv1d(x, self.T2, padding = self.ksize2//2)\n",
    "        if self.maxpool2:\n",
    "            x = F.max_pool1d(x, 2)\n",
    "        y = torch.mm(self.T,x.view(x.shape[0],-1).t())\n",
    "        return x, y\n",
    "        \n",
    "          \n",
    "    def get_params(self):\n",
    "        return self.T1, self.T2, self.X, self.T\n",
    "    \n",
    "    \n",
    "    def X_step(self):\n",
    "        self.X.data = torch.clamp(self.X.data, min=0)\n",
    "\n",
    "\n",
    "    def Z_step(self):\n",
    "        self.Z.data = torch.clamp(self.Z.data, min=0)\n",
    "        \n",
    "        \n",
    "    def get_TZ_Dims(self):\n",
    "        return self.num_features,self.num_atoms, self.input_shape[0]\n",
    "        \n",
    "        \n",
    "class Network(nn.Module): \n",
    "    def __init__(self,inputs_shape=(4,5,1),out_planes1 = 8, out_planes2 = 16,ksize1 = 3,ksize2 = 3,\n",
    "             maxpool1=False, maxpool2=False, mpl_ksize=2,num_classes=2):\n",
    "        super(Network, self).__init__()\n",
    "        self.Transform1 = Transform(inputs_shape,out_planes1 = out_planes1, out_planes2 = out_planes2,ksize1 = ksize1,\n",
    "                                    ksize2 = ksize2,maxpool1=maxpool1, maxpool2=maxpool2, mpl_ksize=mpl_ksize)\n",
    "        self.Transform2 = Transform(inputs_shape,out_planes1 = out_planes1, out_planes2 = out_planes2,ksize1 = ksize1,\n",
    "                                    ksize2 = ksize2,maxpool1=maxpool1, maxpool2=maxpool2, mpl_ksize=mpl_ksize)\n",
    "        self.Transform3 = Transform(inputs_shape,out_planes1 = out_planes1, out_planes2 = out_planes2,ksize1 = ksize1,\n",
    "                                    ksize2 = ksize2,maxpool1=maxpool1, maxpool2=maxpool2, mpl_ksize=mpl_ksize)\n",
    "        self.Transform4 = Transform(inputs_shape,out_planes1 = out_planes1, out_planes2 = out_planes2,ksize1 = ksize1,\n",
    "                                    ksize2 = ksize2,maxpool1=maxpool1, maxpool2=maxpool2, mpl_ksize=mpl_ksize)\n",
    "        self.Transform5 = Transform(inputs_shape,out_planes1 = out_planes1, out_planes2 = out_planes2,ksize1 = ksize1,\n",
    "                                    ksize2 = ksize2,maxpool1=maxpool1, maxpool2=maxpool2, mpl_ksize=mpl_ksize)\n",
    "        self.num_features,self.num_atoms, self.input_shape = self.Transform1.get_TZ_Dims()\n",
    "        Z_shape = [self.num_atoms,self.input_shape]\n",
    "        self.Z = nn.Parameter(torch.randn(Z_shape), requires_grad=True)\n",
    "        self.pred_list = []\n",
    "        self.init_TX()\n",
    "        \n",
    "\n",
    "    def init_TX(self):\n",
    "        self.T11,self.T21, self.X1, self.Tp1 = self.Transform1.get_params()\n",
    "        self.T12,self.T22, self.X2, self.Tp2 = self.Transform2.get_params()\n",
    "        self.T13,self.T23, self.X3, self.Tp3 = self.Transform3.get_params()\n",
    "        self.T14,self.T24, self.X4, self.Tp4 = self.Transform4.get_params()\n",
    "        self.T15,self.T25, self.X5, self.Tp5 = self.Transform5.get_params()\n",
    "        self.T1 = torch.stack((self.T11,self.T12,self.T13,self.T14,self.T15),1)\n",
    "        self.T2 = torch.stack((self.T21,self.T22,self.T23,self.T24,self.T25),1)\n",
    "        self.X = torch.stack((self.X1,self.X2,self.X3,self.X4,self.X5),1) \n",
    "        self.T = torch.stack((self.Tp1,self.Tp2,self.Tp3,self.Tp4,self.Tp5),1) \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size, no_of_series, no_of_days = x.shape\n",
    "        \n",
    "        close = np.reshape(x[:,0],(batch_size,1,no_of_days))\n",
    "        out1,out1p = self.Transform1(close)\n",
    "        \n",
    "        _open = np.reshape(x[:,1],(batch_size,1,no_of_days))\n",
    "        out2,out2p = self.Transform2(_open)\n",
    "        \n",
    "        \n",
    "        high = np.reshape(x[:,2],(batch_size,1,no_of_days))\n",
    "        out3,out3p = self.Transform3(high)\n",
    "        \n",
    "        low = np.reshape(x[:,3],(batch_size,1,no_of_days))\n",
    "        out4,out4p = self.Transform4(low)\n",
    "        \n",
    "        volume = np.reshape(x[:,4],(batch_size,1,no_of_days))\n",
    "        out5, out5p = self.Transform5(volume)\n",
    "        \n",
    "        self.pred_list = [out1,out2,out3,out4,out5]\n",
    "\n",
    "        gp1 = out1p + out2p + out3p + out4p + out5p\n",
    "        return gp1\n",
    "    \n",
    "    \n",
    "    def X_step(self):\n",
    "        self.Transform1.X_step()\n",
    "        self.Transform2.X_step()\n",
    "        self.Transform3.X_step()\n",
    "        self.Transform4.X_step()\n",
    "        self.Transform5.X_step()\n",
    "        \n",
    "        \n",
    "    def Z_step(self):\n",
    "        self.Z.data = torch.clamp(self.Z.data, min=0)\n",
    "        \n",
    "    \n",
    "    def conv_loss_distance(self):\n",
    "        self.init_TX()\n",
    "        \n",
    "        loss = 0.0\n",
    "        X_list = [self.X1,self.X2,self.X3,self.X4,self.X5]\n",
    "        for i in range(len(self.pred_list)): \n",
    "            X = X_list[i].view(X_list[i].size(0), -1)\n",
    "            predictions = self.pred_list[i].view(self.pred_list[i].size(0), -1)\n",
    "            Y = predictions - X\n",
    "            loss += Y.pow(2).mean()\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def conv_loss_logdet(self):\n",
    "\n",
    "        loss = 0.0\n",
    "        for T in [self.T11,self.T21,self.T12,self.T22,self.T13,self.T23,self.T14,self.T24,self.T15,self.T25]:\n",
    "            T = T.view(T.shape[0],-1)\n",
    "            U, s, V = torch.svd(T)\n",
    "            loss += -s.log().sum()\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def conv_loss_frobenius(self):\n",
    "        loss = 0.0\n",
    "        for T in [self.T11,self.T21,self.T12,self.T22,self.T13,self.T23,self.T14,self.T24,self.T15,self.T25]:\n",
    "            loss += T.pow(2).sum()\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def loss_distance(self,predictions):\n",
    "\n",
    "        loss = 0.0\n",
    "        Y = predictions - self.Z\n",
    "        loss += Y.pow(2).mean()    \n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def loss_logdet(self):\n",
    "        loss = 0.0\n",
    "        T = torch.stack((self.Tp1,self.Tp2,self.Tp3,self.Tp4,self.Tp5),1)\n",
    "        T = T.view(T.shape[0],-1)\n",
    "        U, s, V = torch.svd(T)\n",
    "        loss = -s.log().sum()\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def loss_frobenius(self):\n",
    "        loss = 0.0\n",
    "        t_p = torch.stack((self.Tp1,self.Tp2,self.Tp3,self.Tp4,self.Tp5),1)\n",
    "        loss = t_p.pow(2).sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def computeLoss(self,predictions,mu,eps,lam):\n",
    "        loss1 = self.conv_loss_distance()\n",
    "        loss2 = self.conv_loss_frobenius() * eps\n",
    "        loss3 = self.conv_loss_logdet() * mu\n",
    "        loss4 = self.loss_distance(predictions)\n",
    "        loss5 = self.loss_frobenius() * eps\n",
    "        loss6 = self.loss_logdet() * mu\n",
    "        loss = loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def getTZ(self):\n",
    "        return self.T.view(self.T.shape[0],-1), self.Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Related Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, model, optimizer, train_loader, batch_size, mu, eps, lam):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    final_loss = 0\n",
    "    i = 0 \n",
    "    j = 0\n",
    "    T_list = []\n",
    "    Z_list = []\n",
    "    for batch_idx, (X,future_prices) in enumerate(train_loader):\n",
    "        #print('batch:',batch_idx)\n",
    "        data,future_prices = map(lambda x: Variable(x), [X,future_prices])\n",
    "        data_size1 = data.shape[0]\n",
    "        if j == 0: \n",
    "            prev_data = data\n",
    "            prev_future_prices = future_prices\n",
    "            j += 1\n",
    "        if data.shape[0]<batch_size:\n",
    "            diff = batch_size - data.shape[0]\n",
    "            temp_data,temp_labels = prev_data[-diff:,:,:], prev_future_prices[-diff:]\n",
    "            i = 1\n",
    "            data, temp_future_prices = torch.cat((data,temp_data),0),torch.cat((future_prices,temp_future_prices),0)\n",
    "            print('appended data')\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        \n",
    "        final_output = output\n",
    "        \n",
    "        loss = model.computeLoss(final_output,mu,eps,lam)\n",
    "        if epoch%plot_epoch_interval==0:\n",
    "            train_loss.append(loss)\n",
    "            epochs_list.append(epoch)\n",
    "        final_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.X_step()\n",
    "        model.Z_step()\n",
    "        prev_data = data\n",
    "        prev_future_prices = future_prices\n",
    "    print('Epoch : {} , Training loss : {:.4f}\\n'.format(epoch, final_loss.item()))\n",
    "    return train_loss#, T_list, Z_list\n",
    "\n",
    " \n",
    "    \n",
    "def train_on_batch(lr,epochs,momentum,X_train,Y_train,X_test,Y_test,batch_size):\n",
    "    print('seed:',seed)\n",
    "    cuda = False\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    train_loader = DataLoader(RegFinancialData(X_train,Y_train),batch_size=batch_size,shuffle=True) \n",
    "    test_loader = DataLoader(RegFinancialData(X_test,Y_test),batch_size=batch_size,shuffle=False) \n",
    "    \n",
    "    \n",
    "    mu = 0.01\n",
    "    eps = 0.0001\n",
    "    lam = 0 \n",
    "    out_planes1 = out_pl1\n",
    "    out_planes2 = out_pl2\n",
    "    ksize1 = ks1\n",
    "    ksize2 = ks2\n",
    "    maxpool1 = maxpl1\n",
    "    maxpool2 = maxpl2\n",
    "    mpl_ksize = mpl_ks\n",
    "    model = Network(inputs_shape=(batch_size,1,window_size),out_planes1 = out_planes1,out_planes2 = out_planes2,  \n",
    "                    ksize1 = ksize1,ksize2 = ksize2, maxpool1 = maxpool1, maxpool2 = maxpool2, mpl_ksize=mpl_ksize)\n",
    "#     for params in model.parameters():\n",
    "#         print(params)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=5e-5, \n",
    "                                 amsgrad=False)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_model(epoch, model, optimizer, train_loader, batch_size, mu, eps, lam)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    S_train = Variable(torch.from_numpy(X_train).float(), requires_grad=False)\n",
    "    S_test  = Variable(torch.from_numpy(X_test).float(), requires_grad=False)\n",
    "    Z_train =  model(S_train).cpu().data.numpy()\n",
    "    Z_test  = model(S_test).cpu().data.numpy()\n",
    "    print('*'*100)\n",
    "    print(\"Shape of Z_train: \" + str(Z_train.shape))\n",
    "    print(\"Shape of Z_test:  \" + str(Z_test.shape))\n",
    "    print('*'*100)\n",
    "\n",
    "    return Z_train.transpose(),Z_test.transpose(),train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>SYMBOL</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>OPEN_INT</th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CONTRACTS</th>\n",
       "      <th>CHG_IN_OI</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABIRLANUVO</td>\n",
       "      <td>1247.80</td>\n",
       "      <td>627750</td>\n",
       "      <td>1253.05</td>\n",
       "      <td>1259.00</td>\n",
       "      <td>1243.00</td>\n",
       "      <td>448</td>\n",
       "      <td>-9750</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ACC</td>\n",
       "      <td>1119.20</td>\n",
       "      <td>912750</td>\n",
       "      <td>1119.15</td>\n",
       "      <td>1123.85</td>\n",
       "      <td>1109.50</td>\n",
       "      <td>789</td>\n",
       "      <td>-250</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADANIENT</td>\n",
       "      <td>271.55</td>\n",
       "      <td>3606000</td>\n",
       "      <td>266.00</td>\n",
       "      <td>272.30</td>\n",
       "      <td>264.35</td>\n",
       "      <td>1248</td>\n",
       "      <td>62000</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ADANIPORTS</td>\n",
       "      <td>158.05</td>\n",
       "      <td>5568000</td>\n",
       "      <td>157.30</td>\n",
       "      <td>159.35</td>\n",
       "      <td>157.30</td>\n",
       "      <td>585</td>\n",
       "      <td>356000</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ADANIPOWER</td>\n",
       "      <td>39.85</td>\n",
       "      <td>28656000</td>\n",
       "      <td>39.65</td>\n",
       "      <td>40.00</td>\n",
       "      <td>39.35</td>\n",
       "      <td>719</td>\n",
       "      <td>104000</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      SYMBOL    CLOSE  OPEN_INT     OPEN     HIGH      LOW  \\\n",
       "0           0  ABIRLANUVO  1247.80    627750  1253.05  1259.00  1243.00   \n",
       "1           1         ACC  1119.20    912750  1119.15  1123.85  1109.50   \n",
       "2           2    ADANIENT   271.55   3606000   266.00   272.30   264.35   \n",
       "3           3  ADANIPORTS   158.05   5568000   157.30   159.35   157.30   \n",
       "4           4  ADANIPOWER    39.85  28656000    39.65    40.00    39.35   \n",
       "\n",
       "   CONTRACTS  CHG_IN_OI        DATE  \n",
       "0        448      -9750  2014-01-01  \n",
       "1        789       -250  2014-01-01  \n",
       "2       1248      62000  2014-01-01  \n",
       "3        585     356000  2014-01-01  \n",
       "4        719     104000  2014-01-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "window_size = 5\n",
    "fileName = 'phd_research_data.csv'\n",
    "data_df = getData(fileName)\n",
    "if fileName == 'phd_research_data.csv':\n",
    "    data_df.drop(['Unnamed: 0'],inplace=True,axis=1)\n",
    "data_df,labels_df = labelData(data_df.copy())\n",
    "data = np.asarray(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length :  150\n"
     ]
    }
   ],
   "source": [
    "stocks_list = getStocksList(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkClassImbal(Y_train):\n",
    "    Ytrain_df= pd.DataFrame(Y_train,columns=[0])\n",
    "    print(Ytrain_df.shape)\n",
    "    print(Ytrain_df.columns)\n",
    "    df = Ytrain_df.groupby(0).size()\n",
    "    print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 150\n",
    "seed_range = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "train_accuracies = []\n",
    "epochs_list = []\n",
    "learning_rates = []\n",
    "epoch_interval = 10\n",
    "plot_epoch_interval = 5\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "epochs = 100\n",
    "test_size = 0.2\n",
    "features_list = ['CLOSE','OPEN','HIGH','LOW','CONTRACTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_op1_4_op2_8_mp1_True_mp2_False_ks1_5_ks2_3_new\n"
     ]
    }
   ],
   "source": [
    "out_pl1 = 4\n",
    "out_pl2 = 8\n",
    "maxpl1 = True\n",
    "maxpl2 = False\n",
    "ks1 = 5\n",
    "ks2 = 3\n",
    "mpl_ks = 2\n",
    "custom_batch_size_flag = False\n",
    "bs = 32\n",
    "if custom_batch_size_flag == True:\n",
    "    param_path = '_op1_' + str(out_pl1) + '_op2_' + str(out_pl2) \\\n",
    "            +'_mp1_' + str(maxpl1) + '_mp2_' + str(maxpl2) + '_ks1_' + str(ks1) + '_ks2_' + str(ks2) \\\n",
    "            + '_bs_' + str(bs) + '_new'\n",
    "else:\n",
    "    param_path = '_op1_' + str(out_pl1) + '_op2_' + str(out_pl2) \\\n",
    "                +'_mp1_' + str(maxpl1) + '_mp2_' + str(maxpl2) + '_ks1_' + str(ks1) + '_ks2_' + str(ks2) + '_new'\n",
    "print(param_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev_day_values.shape: (171,)\n",
      "X_test.shape: (171, 5, 5)\n",
      "Y_test.shape: (171,)\n",
      "next_day_values.shape: (855,)\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  1\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 1\n",
      "Epoch : 1 , Training loss : 7059385.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 6717116.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 6392193.5000\n",
      "\n",
      "Epoch : 4 , Training loss : 6083825.5000\n",
      "\n",
      "Epoch : 5 , Training loss : 5791593.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 5514709.5000\n",
      "\n",
      "Epoch : 7 , Training loss : 5252405.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 5004113.5000\n",
      "\n",
      "Epoch : 9 , Training loss : 4769313.5000\n",
      "\n",
      "Epoch : 10 , Training loss : 4547394.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 4337705.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 4139599.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 3952617.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 3776108.7500\n",
      "\n",
      "Epoch : 15 , Training loss : 3609572.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 3452510.7500\n",
      "\n",
      "Epoch : 17 , Training loss : 3304424.0000\n",
      "\n",
      "Epoch : 18 , Training loss : 3164860.5000\n",
      "\n",
      "Epoch : 19 , Training loss : 3033356.0000\n",
      "\n",
      "Epoch : 20 , Training loss : 2909343.0000\n",
      "\n",
      "Epoch : 21 , Training loss : 2792365.0000\n",
      "\n",
      "Epoch : 22 , Training loss : 2681916.0000\n",
      "\n",
      "Epoch : 23 , Training loss : 2577598.5000\n",
      "\n",
      "Epoch : 24 , Training loss : 2478989.7500\n",
      "\n",
      "Epoch : 25 , Training loss : 2385519.5000\n",
      "\n",
      "Epoch : 26 , Training loss : 2296860.5000\n",
      "\n",
      "Epoch : 27 , Training loss : 2212769.2500\n",
      "\n",
      "Epoch : 28 , Training loss : 2132986.7500\n",
      "\n",
      "Epoch : 29 , Training loss : 2057141.5000\n",
      "\n",
      "Epoch : 30 , Training loss : 1984940.8750\n",
      "\n",
      "Epoch : 31 , Training loss : 1916288.6250\n",
      "\n",
      "Epoch : 32 , Training loss : 1851050.7500\n",
      "\n",
      "Epoch : 33 , Training loss : 1789055.2500\n",
      "\n",
      "Epoch : 34 , Training loss : 1730097.3750\n",
      "\n",
      "Epoch : 35 , Training loss : 1674114.3750\n",
      "\n",
      "Epoch : 36 , Training loss : 1620834.8750\n",
      "\n",
      "Epoch : 37 , Training loss : 1570141.8750\n",
      "\n",
      "Epoch : 38 , Training loss : 1521933.1250\n",
      "\n",
      "Epoch : 39 , Training loss : 1476095.6250\n",
      "\n",
      "Epoch : 40 , Training loss : 1432496.1250\n",
      "\n",
      "Epoch : 41 , Training loss : 1390999.2500\n",
      "\n",
      "Epoch : 42 , Training loss : 1351553.0000\n",
      "\n",
      "Epoch : 43 , Training loss : 1314034.3750\n",
      "\n",
      "Epoch : 44 , Training loss : 1278504.3750\n",
      "\n",
      "Epoch : 45 , Training loss : 1244769.1250\n",
      "\n",
      "Epoch : 46 , Training loss : 1212580.6250\n",
      "\n",
      "Epoch : 47 , Training loss : 1181841.0000\n",
      "\n",
      "Epoch : 48 , Training loss : 1152480.0000\n",
      "\n",
      "Epoch : 49 , Training loss : 1124408.2500\n",
      "\n",
      "Epoch : 50 , Training loss : 1097521.0000\n",
      "\n",
      "Epoch : 51 , Training loss : 1071745.2500\n",
      "\n",
      "Epoch : 52 , Training loss : 1047053.1250\n",
      "\n",
      "Epoch : 53 , Training loss : 1023344.8125\n",
      "\n",
      "Epoch : 54 , Training loss : 1000594.0000\n",
      "\n",
      "Epoch : 55 , Training loss : 978713.7500\n",
      "\n",
      "Epoch : 56 , Training loss : 957702.1875\n",
      "\n",
      "Epoch : 57 , Training loss : 937496.6250\n",
      "\n",
      "Epoch : 58 , Training loss : 918085.6250\n",
      "\n",
      "Epoch : 59 , Training loss : 899430.6250\n",
      "\n",
      "Epoch : 60 , Training loss : 881445.0000\n",
      "\n",
      "Epoch : 61 , Training loss : 864171.3125\n",
      "\n",
      "Epoch : 62 , Training loss : 847516.9375\n",
      "\n",
      "Epoch : 63 , Training loss : 831440.8750\n",
      "\n",
      "Epoch : 64 , Training loss : 815904.6875\n",
      "\n",
      "Epoch : 65 , Training loss : 800896.3750\n",
      "\n",
      "Epoch : 66 , Training loss : 786368.3750\n",
      "\n",
      "Epoch : 67 , Training loss : 772312.5625\n",
      "\n",
      "Epoch : 68 , Training loss : 758693.7500\n",
      "\n",
      "Epoch : 69 , Training loss : 745501.7500\n",
      "\n",
      "Epoch : 70 , Training loss : 732742.2500\n",
      "\n",
      "Epoch : 71 , Training loss : 720353.0625\n",
      "\n",
      "Epoch : 72 , Training loss : 708328.6875\n",
      "\n",
      "Epoch : 73 , Training loss : 696637.1875\n",
      "\n",
      "Epoch : 74 , Training loss : 685304.7500\n",
      "\n",
      "Epoch : 75 , Training loss : 674273.3750\n",
      "\n",
      "Epoch : 76 , Training loss : 663548.0000\n",
      "\n",
      "Epoch : 77 , Training loss : 653097.8750\n",
      "\n",
      "Epoch : 78 , Training loss : 642915.2500\n",
      "\n",
      "Epoch : 79 , Training loss : 632998.6250\n",
      "\n",
      "Epoch : 80 , Training loss : 623327.2500\n",
      "\n",
      "Epoch : 81 , Training loss : 613917.6875\n",
      "\n",
      "Epoch : 82 , Training loss : 604716.8125\n",
      "\n",
      "Epoch : 83 , Training loss : 595750.8750\n",
      "\n",
      "Epoch : 84 , Training loss : 587016.3750\n",
      "\n",
      "Epoch : 85 , Training loss : 578486.8125\n",
      "\n",
      "Epoch : 86 , Training loss : 570168.5625\n",
      "\n",
      "Epoch : 87 , Training loss : 562042.5625\n",
      "\n",
      "Epoch : 88 , Training loss : 554101.3125\n",
      "\n",
      "Epoch : 89 , Training loss : 546339.1250\n",
      "\n",
      "Epoch : 90 , Training loss : 538754.7500\n",
      "\n",
      "Epoch : 91 , Training loss : 531339.6875\n",
      "\n",
      "Epoch : 92 , Training loss : 524089.4688\n",
      "\n",
      "Epoch : 93 , Training loss : 516989.3750\n",
      "\n",
      "Epoch : 94 , Training loss : 510047.8750\n",
      "\n",
      "Epoch : 95 , Training loss : 503262.5938\n",
      "\n",
      "Epoch : 96 , Training loss : 496601.7500\n",
      "\n",
      "Epoch : 97 , Training loss : 490091.7500\n",
      "\n",
      "Epoch : 98 , Training loss : 483717.3125\n",
      "\n",
      "Epoch : 99 , Training loss : 477475.2500\n",
      "\n",
      "Epoch : 100 , Training loss : 471351.4375\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.939057\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  2\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 2\n",
      "Epoch : 1 , Training loss : 7114814.5000\n",
      "\n",
      "Epoch : 2 , Training loss : 6779750.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 6458446.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 6150849.5000\n",
      "\n",
      "Epoch : 5 , Training loss : 5856716.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 5575969.5000\n",
      "\n",
      "Epoch : 7 , Training loss : 5308565.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 5054132.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 4812166.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 4582128.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 4363445.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 4155593.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 3958056.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 3770434.2500\n",
      "\n",
      "Epoch : 15 , Training loss : 3592405.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 3423654.5000\n",
      "\n",
      "Epoch : 17 , Training loss : 3263913.0000\n",
      "\n",
      "Epoch : 18 , Training loss : 3112962.0000\n",
      "\n",
      "Epoch : 19 , Training loss : 2970236.0000\n",
      "\n",
      "Epoch : 20 , Training loss : 2835182.0000\n",
      "\n",
      "Epoch : 21 , Training loss : 2707142.5000\n",
      "\n",
      "Epoch : 22 , Training loss : 2585705.7500\n",
      "\n",
      "Epoch : 23 , Training loss : 2470450.5000\n",
      "\n",
      "Epoch : 24 , Training loss : 2361011.5000\n",
      "\n",
      "Epoch : 25 , Training loss : 2257042.7500\n",
      "\n",
      "Epoch : 26 , Training loss : 2158260.7500\n",
      "\n",
      "Epoch : 27 , Training loss : 2064357.8750\n",
      "\n",
      "Epoch : 28 , Training loss : 1975067.8750\n",
      "\n",
      "Epoch : 29 , Training loss : 1890161.0000\n",
      "\n",
      "Epoch : 30 , Training loss : 1809379.7500\n",
      "\n",
      "Epoch : 31 , Training loss : 1732517.8750\n",
      "\n",
      "Epoch : 32 , Training loss : 1659367.7500\n",
      "\n",
      "Epoch : 33 , Training loss : 1589703.8750\n",
      "\n",
      "Epoch : 34 , Training loss : 1523349.7500\n",
      "\n",
      "Epoch : 35 , Training loss : 1460147.7500\n",
      "\n",
      "Epoch : 36 , Training loss : 1399901.1250\n",
      "\n",
      "Epoch : 37 , Training loss : 1342484.6250\n",
      "\n",
      "Epoch : 38 , Training loss : 1287751.2500\n",
      "\n",
      "Epoch : 39 , Training loss : 1235522.6250\n",
      "\n",
      "Epoch : 40 , Training loss : 1185685.8750\n",
      "\n",
      "Epoch : 41 , Training loss : 1138110.3750\n",
      "\n",
      "Epoch : 42 , Training loss : 1092683.5000\n",
      "\n",
      "Epoch : 43 , Training loss : 1049302.8750\n",
      "\n",
      "Epoch : 44 , Training loss : 1007869.7500\n",
      "\n",
      "Epoch : 45 , Training loss : 968298.5000\n",
      "\n",
      "Epoch : 46 , Training loss : 930494.2500\n",
      "\n",
      "Epoch : 47 , Training loss : 894378.1250\n",
      "\n",
      "Epoch : 48 , Training loss : 859868.1250\n",
      "\n",
      "Epoch : 49 , Training loss : 826897.8125\n",
      "\n",
      "Epoch : 50 , Training loss : 795400.3750\n",
      "\n",
      "Epoch : 51 , Training loss : 765311.9375\n",
      "\n",
      "Epoch : 52 , Training loss : 736574.5000\n",
      "\n",
      "Epoch : 53 , Training loss : 709125.5625\n",
      "\n",
      "Epoch : 54 , Training loss : 682900.0000\n",
      "\n",
      "Epoch : 55 , Training loss : 657861.5625\n",
      "\n",
      "Epoch : 56 , Training loss : 633944.5625\n",
      "\n",
      "Epoch : 57 , Training loss : 611110.3125\n",
      "\n",
      "Epoch : 58 , Training loss : 589309.7500\n",
      "\n",
      "Epoch : 59 , Training loss : 568499.8750\n",
      "\n",
      "Epoch : 60 , Training loss : 548648.5625\n",
      "\n",
      "Epoch : 61 , Training loss : 529694.1875\n",
      "\n",
      "Epoch : 62 , Training loss : 511605.8750\n",
      "\n",
      "Epoch : 63 , Training loss : 494341.8750\n",
      "\n",
      "Epoch : 64 , Training loss : 477884.4688\n",
      "\n",
      "Epoch : 65 , Training loss : 462177.8750\n",
      "\n",
      "Epoch : 66 , Training loss : 447200.0000\n",
      "\n",
      "Epoch : 67 , Training loss : 432913.9688\n",
      "\n",
      "Epoch : 68 , Training loss : 419293.8125\n",
      "\n",
      "Epoch : 69 , Training loss : 406305.6875\n",
      "\n",
      "Epoch : 70 , Training loss : 393931.6875\n",
      "\n",
      "Epoch : 71 , Training loss : 382135.5312\n",
      "\n",
      "Epoch : 72 , Training loss : 370898.2500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 73 , Training loss : 360192.6875\n",
      "\n",
      "Epoch : 74 , Training loss : 350005.5000\n",
      "\n",
      "Epoch : 75 , Training loss : 340297.9375\n",
      "\n",
      "Epoch : 76 , Training loss : 331066.3438\n",
      "\n",
      "Epoch : 77 , Training loss : 322276.4375\n",
      "\n",
      "Epoch : 78 , Training loss : 313920.1875\n",
      "\n",
      "Epoch : 79 , Training loss : 305970.8125\n",
      "\n",
      "Epoch : 80 , Training loss : 298408.0312\n",
      "\n",
      "Epoch : 81 , Training loss : 291222.1562\n",
      "\n",
      "Epoch : 82 , Training loss : 284392.7188\n",
      "\n",
      "Epoch : 83 , Training loss : 277897.3438\n",
      "\n",
      "Epoch : 84 , Training loss : 271725.8125\n",
      "\n",
      "Epoch : 85 , Training loss : 265863.5938\n",
      "\n",
      "Epoch : 86 , Training loss : 260288.5938\n",
      "\n",
      "Epoch : 87 , Training loss : 254995.2812\n",
      "\n",
      "Epoch : 88 , Training loss : 249965.9688\n",
      "\n",
      "Epoch : 89 , Training loss : 245189.0000\n",
      "\n",
      "Epoch : 90 , Training loss : 240646.3438\n",
      "\n",
      "Epoch : 91 , Training loss : 236330.1875\n",
      "\n",
      "Epoch : 92 , Training loss : 232226.5625\n",
      "\n",
      "Epoch : 93 , Training loss : 228327.3438\n",
      "\n",
      "Epoch : 94 , Training loss : 224616.1875\n",
      "\n",
      "Epoch : 95 , Training loss : 221085.9375\n",
      "\n",
      "Epoch : 96 , Training loss : 217725.9688\n",
      "\n",
      "Epoch : 97 , Training loss : 214532.5312\n",
      "\n",
      "Epoch : 98 , Training loss : 211486.9531\n",
      "\n",
      "Epoch : 99 , Training loss : 208581.5625\n",
      "\n",
      "Epoch : 100 , Training loss : 205814.7031\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.613841\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  3\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 3\n",
      "Epoch : 1 , Training loss : 8053508.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 7691521.5000\n",
      "\n",
      "Epoch : 3 , Training loss : 7343728.5000\n",
      "\n",
      "Epoch : 4 , Training loss : 7010454.5000\n",
      "\n",
      "Epoch : 5 , Training loss : 6691387.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 6386124.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 6094256.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 5815420.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 5549189.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 5295273.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 5053269.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 4822856.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 4603595.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 4395219.0000\n",
      "\n",
      "Epoch : 15 , Training loss : 4197267.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 4009325.5000\n",
      "\n",
      "Epoch : 17 , Training loss : 3831010.0000\n",
      "\n",
      "Epoch : 18 , Training loss : 3661798.0000\n",
      "\n",
      "Epoch : 19 , Training loss : 3501347.5000\n",
      "\n",
      "Epoch : 20 , Training loss : 3349225.0000\n",
      "\n",
      "Epoch : 21 , Training loss : 3204882.0000\n",
      "\n",
      "Epoch : 22 , Training loss : 3067942.0000\n",
      "\n",
      "Epoch : 23 , Training loss : 2937898.7500\n",
      "\n",
      "Epoch : 24 , Training loss : 2814530.2500\n",
      "\n",
      "Epoch : 25 , Training loss : 2697497.5000\n",
      "\n",
      "Epoch : 26 , Training loss : 2586560.0000\n",
      "\n",
      "Epoch : 27 , Training loss : 2481528.0000\n",
      "\n",
      "Epoch : 28 , Training loss : 2382109.7500\n",
      "\n",
      "Epoch : 29 , Training loss : 2288057.5000\n",
      "\n",
      "Epoch : 30 , Training loss : 2199099.7500\n",
      "\n",
      "Epoch : 31 , Training loss : 2115051.2500\n",
      "\n",
      "Epoch : 32 , Training loss : 2035605.2500\n",
      "\n",
      "Epoch : 33 , Training loss : 1960479.1250\n",
      "\n",
      "Epoch : 34 , Training loss : 1889435.2500\n",
      "\n",
      "Epoch : 35 , Training loss : 1822234.8750\n",
      "\n",
      "Epoch : 36 , Training loss : 1758634.1250\n",
      "\n",
      "Epoch : 37 , Training loss : 1698411.3750\n",
      "\n",
      "Epoch : 38 , Training loss : 1641375.2500\n",
      "\n",
      "Epoch : 39 , Training loss : 1587455.5000\n",
      "\n",
      "Epoch : 40 , Training loss : 1536394.0000\n",
      "\n",
      "Epoch : 41 , Training loss : 1488029.5000\n",
      "\n",
      "Epoch : 42 , Training loss : 1442192.5000\n",
      "\n",
      "Epoch : 43 , Training loss : 1398704.2500\n",
      "\n",
      "Epoch : 44 , Training loss : 1357418.1250\n",
      "\n",
      "Epoch : 45 , Training loss : 1318213.2500\n",
      "\n",
      "Epoch : 46 , Training loss : 1280928.3750\n",
      "\n",
      "Epoch : 47 , Training loss : 1245471.1250\n",
      "\n",
      "Epoch : 48 , Training loss : 1211654.6250\n",
      "\n",
      "Epoch : 49 , Training loss : 1179434.1250\n",
      "\n",
      "Epoch : 50 , Training loss : 1148670.8750\n",
      "\n",
      "Epoch : 51 , Training loss : 1119288.2500\n",
      "\n",
      "Epoch : 52 , Training loss : 1091213.2500\n",
      "\n",
      "Epoch : 53 , Training loss : 1064383.1250\n",
      "\n",
      "Epoch : 54 , Training loss : 1038694.0000\n",
      "\n",
      "Epoch : 55 , Training loss : 1014077.3125\n",
      "\n",
      "Epoch : 56 , Training loss : 990499.5000\n",
      "\n",
      "Epoch : 57 , Training loss : 967899.4375\n",
      "\n",
      "Epoch : 58 , Training loss : 946220.1250\n",
      "\n",
      "Epoch : 59 , Training loss : 925402.4375\n",
      "\n",
      "Epoch : 60 , Training loss : 905436.0000\n",
      "\n",
      "Epoch : 61 , Training loss : 886256.5000\n",
      "\n",
      "Epoch : 62 , Training loss : 867857.9375\n",
      "\n",
      "Epoch : 63 , Training loss : 850162.3750\n",
      "\n",
      "Epoch : 64 , Training loss : 833151.8125\n",
      "\n",
      "Epoch : 65 , Training loss : 816798.0625\n",
      "\n",
      "Epoch : 66 , Training loss : 801055.0000\n",
      "\n",
      "Epoch : 67 , Training loss : 785897.9375\n",
      "\n",
      "Epoch : 68 , Training loss : 771301.5625\n",
      "\n",
      "Epoch : 69 , Training loss : 757221.1875\n",
      "\n",
      "Epoch : 70 , Training loss : 743669.2500\n",
      "\n",
      "Epoch : 71 , Training loss : 730572.6875\n",
      "\n",
      "Epoch : 72 , Training loss : 717941.4375\n",
      "\n",
      "Epoch : 73 , Training loss : 705754.1875\n",
      "\n",
      "Epoch : 74 , Training loss : 693968.2500\n",
      "\n",
      "Epoch : 75 , Training loss : 682575.3125\n",
      "\n",
      "Epoch : 76 , Training loss : 671546.9375\n",
      "\n",
      "Epoch : 77 , Training loss : 660842.1250\n",
      "\n",
      "Epoch : 78 , Training loss : 650454.4375\n",
      "\n",
      "Epoch : 79 , Training loss : 640397.6875\n",
      "\n",
      "Epoch : 80 , Training loss : 630633.0000\n",
      "\n",
      "Epoch : 81 , Training loss : 621158.3750\n",
      "\n",
      "Epoch : 82 , Training loss : 611953.7500\n",
      "\n",
      "Epoch : 83 , Training loss : 603011.0000\n",
      "\n",
      "Epoch : 84 , Training loss : 594332.2500\n",
      "\n",
      "Epoch : 85 , Training loss : 585880.9375\n",
      "\n",
      "Epoch : 86 , Training loss : 577669.1875\n",
      "\n",
      "Epoch : 87 , Training loss : 569673.7500\n",
      "\n",
      "Epoch : 88 , Training loss : 561887.6875\n",
      "\n",
      "Epoch : 89 , Training loss : 554314.4375\n",
      "\n",
      "Epoch : 90 , Training loss : 546929.1875\n",
      "\n",
      "Epoch : 91 , Training loss : 539725.9375\n",
      "\n",
      "Epoch : 92 , Training loss : 532732.6250\n",
      "\n",
      "Epoch : 93 , Training loss : 525895.3750\n",
      "\n",
      "Epoch : 94 , Training loss : 519235.0625\n",
      "\n",
      "Epoch : 95 , Training loss : 512734.0000\n",
      "\n",
      "Epoch : 96 , Training loss : 506396.1250\n",
      "\n",
      "Epoch : 97 , Training loss : 500197.0625\n",
      "\n",
      "Epoch : 98 , Training loss : 494154.7500\n",
      "\n",
      "Epoch : 99 , Training loss : 488246.2500\n",
      "\n",
      "Epoch : 100 , Training loss : 482475.8125\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.662152\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  4\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 4\n",
      "Epoch : 1 , Training loss : 20591726.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 19623884.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 18697178.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 17811624.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 16967004.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 16162156.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 15396251.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 14667416.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 13973970.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 13314348.0000\n",
      "\n",
      "Epoch : 11 , Training loss : 12686991.0000\n",
      "\n",
      "Epoch : 12 , Training loss : 12090540.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 11523981.0000\n",
      "\n",
      "Epoch : 14 , Training loss : 10986070.0000\n",
      "\n",
      "Epoch : 15 , Training loss : 10475759.0000\n",
      "\n",
      "Epoch : 16 , Training loss : 9991794.0000\n",
      "\n",
      "Epoch : 17 , Training loss : 9533051.0000\n",
      "\n",
      "Epoch : 18 , Training loss : 9098406.0000\n",
      "\n",
      "Epoch : 19 , Training loss : 8686848.0000\n",
      "\n",
      "Epoch : 20 , Training loss : 8297133.5000\n",
      "\n",
      "Epoch : 21 , Training loss : 7928264.0000\n",
      "\n",
      "Epoch : 22 , Training loss : 7579132.5000\n",
      "\n",
      "Epoch : 23 , Training loss : 7248698.0000\n",
      "\n",
      "Epoch : 24 , Training loss : 6935849.0000\n",
      "\n",
      "Epoch : 25 , Training loss : 6639727.5000\n",
      "\n",
      "Epoch : 26 , Training loss : 6359302.5000\n",
      "\n",
      "Epoch : 27 , Training loss : 6093789.5000\n",
      "\n",
      "Epoch : 28 , Training loss : 5842424.0000\n",
      "\n",
      "Epoch : 29 , Training loss : 5604393.5000\n",
      "\n",
      "Epoch : 30 , Training loss : 5378966.5000\n",
      "\n",
      "Epoch : 31 , Training loss : 5165412.5000\n",
      "\n",
      "Epoch : 32 , Training loss : 4963106.0000\n",
      "\n",
      "Epoch : 33 , Training loss : 4771395.0000\n",
      "\n",
      "Epoch : 34 , Training loss : 4589720.5000\n",
      "\n",
      "Epoch : 35 , Training loss : 4417518.5000\n",
      "\n",
      "Epoch : 36 , Training loss : 4254317.5000\n",
      "\n",
      "Epoch : 37 , Training loss : 4099586.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 38 , Training loss : 3952888.5000\n",
      "\n",
      "Epoch : 39 , Training loss : 3813799.5000\n",
      "\n",
      "Epoch : 40 , Training loss : 3681810.2500\n",
      "\n",
      "Epoch : 41 , Training loss : 3556666.5000\n",
      "\n",
      "Epoch : 42 , Training loss : 3437927.0000\n",
      "\n",
      "Epoch : 43 , Training loss : 3325295.0000\n",
      "\n",
      "Epoch : 44 , Training loss : 3218431.0000\n",
      "\n",
      "Epoch : 45 , Training loss : 3117041.2500\n",
      "\n",
      "Epoch : 46 , Training loss : 3020817.2500\n",
      "\n",
      "Epoch : 47 , Training loss : 2929478.0000\n",
      "\n",
      "Epoch : 48 , Training loss : 2842783.2500\n",
      "\n",
      "Epoch : 49 , Training loss : 2760442.2500\n",
      "\n",
      "Epoch : 50 , Training loss : 2682200.0000\n",
      "\n",
      "Epoch : 51 , Training loss : 2607926.5000\n",
      "\n",
      "Epoch : 52 , Training loss : 2537323.2500\n",
      "\n",
      "Epoch : 53 , Training loss : 2470187.5000\n",
      "\n",
      "Epoch : 54 , Training loss : 2406337.2500\n",
      "\n",
      "Epoch : 55 , Training loss : 2345602.5000\n",
      "\n",
      "Epoch : 56 , Training loss : 2287761.7500\n",
      "\n",
      "Epoch : 57 , Training loss : 2232688.0000\n",
      "\n",
      "Epoch : 58 , Training loss : 2180174.2500\n",
      "\n",
      "Epoch : 59 , Training loss : 2130139.7500\n",
      "\n",
      "Epoch : 60 , Training loss : 2082410.6250\n",
      "\n",
      "Epoch : 61 , Training loss : 2036821.5000\n",
      "\n",
      "Epoch : 62 , Training loss : 1993254.0000\n",
      "\n",
      "Epoch : 63 , Training loss : 1951606.3750\n",
      "\n",
      "Epoch : 64 , Training loss : 1911737.8750\n",
      "\n",
      "Epoch : 65 , Training loss : 1873543.5000\n",
      "\n",
      "Epoch : 66 , Training loss : 1836965.7500\n",
      "\n",
      "Epoch : 67 , Training loss : 1801872.1250\n",
      "\n",
      "Epoch : 68 , Training loss : 1768179.5000\n",
      "\n",
      "Epoch : 69 , Training loss : 1735792.7500\n",
      "\n",
      "Epoch : 70 , Training loss : 1704674.3750\n",
      "\n",
      "Epoch : 71 , Training loss : 1674712.0000\n",
      "\n",
      "Epoch : 72 , Training loss : 1645861.6250\n",
      "\n",
      "Epoch : 73 , Training loss : 1618069.0000\n",
      "\n",
      "Epoch : 74 , Training loss : 1591226.7500\n",
      "\n",
      "Epoch : 75 , Training loss : 1565337.0000\n",
      "\n",
      "Epoch : 76 , Training loss : 1540310.0000\n",
      "\n",
      "Epoch : 77 , Training loss : 1516124.0000\n",
      "\n",
      "Epoch : 78 , Training loss : 1492706.6250\n",
      "\n",
      "Epoch : 79 , Training loss : 1470038.6250\n",
      "\n",
      "Epoch : 80 , Training loss : 1448084.1250\n",
      "\n",
      "Epoch : 81 , Training loss : 1426762.1250\n",
      "\n",
      "Epoch : 82 , Training loss : 1406088.2500\n",
      "\n",
      "Epoch : 83 , Training loss : 1385993.5000\n",
      "\n",
      "Epoch : 84 , Training loss : 1366466.6250\n",
      "\n",
      "Epoch : 85 , Training loss : 1347504.6250\n",
      "\n",
      "Epoch : 86 , Training loss : 1329057.0000\n",
      "\n",
      "Epoch : 87 , Training loss : 1311096.0000\n",
      "\n",
      "Epoch : 88 , Training loss : 1293587.0000\n",
      "\n",
      "Epoch : 89 , Training loss : 1276549.7500\n",
      "\n",
      "Epoch : 90 , Training loss : 1259915.3750\n",
      "\n",
      "Epoch : 91 , Training loss : 1243702.2500\n",
      "\n",
      "Epoch : 92 , Training loss : 1227874.6250\n",
      "\n",
      "Epoch : 93 , Training loss : 1212430.1250\n",
      "\n",
      "Epoch : 94 , Training loss : 1197340.6250\n",
      "\n",
      "Epoch : 95 , Training loss : 1182625.1250\n",
      "\n",
      "Epoch : 96 , Training loss : 1168214.8750\n",
      "\n",
      "Epoch : 97 , Training loss : 1154136.5000\n",
      "\n",
      "Epoch : 98 , Training loss : 1140369.5000\n",
      "\n",
      "Epoch : 99 , Training loss : 1126886.7500\n",
      "\n",
      "Epoch : 100 , Training loss : 1113714.5000\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.672553\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  5\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 5\n",
      "Epoch : 1 , Training loss : 8140301.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 7701174.5000\n",
      "\n",
      "Epoch : 3 , Training loss : 7285998.5000\n",
      "\n",
      "Epoch : 4 , Training loss : 6894244.5000\n",
      "\n",
      "Epoch : 5 , Training loss : 6525218.5000\n",
      "\n",
      "Epoch : 6 , Training loss : 6178050.5000\n",
      "\n",
      "Epoch : 7 , Training loss : 5851919.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 5545960.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 5259264.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 4990921.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 4739946.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 4505356.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 4286093.0000\n",
      "\n",
      "Epoch : 14 , Training loss : 4081198.0000\n",
      "\n",
      "Epoch : 15 , Training loss : 3889626.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 3710453.0000\n",
      "\n",
      "Epoch : 17 , Training loss : 3542700.0000\n",
      "\n",
      "Epoch : 18 , Training loss : 3385396.2500\n",
      "\n",
      "Epoch : 19 , Training loss : 3237898.2500\n",
      "\n",
      "Epoch : 20 , Training loss : 3099455.0000\n",
      "\n",
      "Epoch : 21 , Training loss : 2969375.2500\n",
      "\n",
      "Epoch : 22 , Training loss : 2847033.2500\n",
      "\n",
      "Epoch : 23 , Training loss : 2731883.0000\n",
      "\n",
      "Epoch : 24 , Training loss : 2623441.2500\n",
      "\n",
      "Epoch : 25 , Training loss : 2521225.5000\n",
      "\n",
      "Epoch : 26 , Training loss : 2424842.7500\n",
      "\n",
      "Epoch : 27 , Training loss : 2333895.5000\n",
      "\n",
      "Epoch : 28 , Training loss : 2248036.0000\n",
      "\n",
      "Epoch : 29 , Training loss : 2166932.0000\n",
      "\n",
      "Epoch : 30 , Training loss : 2090312.6250\n",
      "\n",
      "Epoch : 31 , Training loss : 2017896.3750\n",
      "\n",
      "Epoch : 32 , Training loss : 1949465.7500\n",
      "\n",
      "Epoch : 33 , Training loss : 1884755.1250\n",
      "\n",
      "Epoch : 34 , Training loss : 1823518.1250\n",
      "\n",
      "Epoch : 35 , Training loss : 1765580.8750\n",
      "\n",
      "Epoch : 36 , Training loss : 1710724.5000\n",
      "\n",
      "Epoch : 37 , Training loss : 1658752.8750\n",
      "\n",
      "Epoch : 38 , Training loss : 1609508.6250\n",
      "\n",
      "Epoch : 39 , Training loss : 1562804.2500\n",
      "\n",
      "Epoch : 40 , Training loss : 1518480.0000\n",
      "\n",
      "Epoch : 41 , Training loss : 1476359.0000\n",
      "\n",
      "Epoch : 42 , Training loss : 1436351.5000\n",
      "\n",
      "Epoch : 43 , Training loss : 1398275.1250\n",
      "\n",
      "Epoch : 44 , Training loss : 1362063.2500\n",
      "\n",
      "Epoch : 45 , Training loss : 1327552.3750\n",
      "\n",
      "Epoch : 46 , Training loss : 1294668.6250\n",
      "\n",
      "Epoch : 47 , Training loss : 1263301.2500\n",
      "\n",
      "Epoch : 48 , Training loss : 1233371.2500\n",
      "\n",
      "Epoch : 49 , Training loss : 1204787.3750\n",
      "\n",
      "Epoch : 50 , Training loss : 1177452.0000\n",
      "\n",
      "Epoch : 51 , Training loss : 1151316.1250\n",
      "\n",
      "Epoch : 52 , Training loss : 1126317.7500\n",
      "\n",
      "Epoch : 53 , Training loss : 1102380.8750\n",
      "\n",
      "Epoch : 54 , Training loss : 1079425.3750\n",
      "\n",
      "Epoch : 55 , Training loss : 1057405.1250\n",
      "\n",
      "Epoch : 56 , Training loss : 1036272.0625\n",
      "\n",
      "Epoch : 57 , Training loss : 1015974.1250\n",
      "\n",
      "Epoch : 58 , Training loss : 996464.9375\n",
      "\n",
      "Epoch : 59 , Training loss : 977714.3125\n",
      "\n",
      "Epoch : 60 , Training loss : 959664.0625\n",
      "\n",
      "Epoch : 61 , Training loss : 942292.6250\n",
      "\n",
      "Epoch : 62 , Training loss : 925527.7500\n",
      "\n",
      "Epoch : 63 , Training loss : 909336.9375\n",
      "\n",
      "Epoch : 64 , Training loss : 893717.0000\n",
      "\n",
      "Epoch : 65 , Training loss : 878637.3750\n",
      "\n",
      "Epoch : 66 , Training loss : 864055.8125\n",
      "\n",
      "Epoch : 67 , Training loss : 849943.3125\n",
      "\n",
      "Epoch : 68 , Training loss : 836309.8750\n",
      "\n",
      "Epoch : 69 , Training loss : 823112.6875\n",
      "\n",
      "Epoch : 70 , Training loss : 810310.2500\n",
      "\n",
      "Epoch : 71 , Training loss : 797906.0000\n",
      "\n",
      "Epoch : 72 , Training loss : 785826.9375\n",
      "\n",
      "Epoch : 73 , Training loss : 774107.6250\n",
      "\n",
      "Epoch : 74 , Training loss : 762716.1875\n",
      "\n",
      "Epoch : 75 , Training loss : 751639.1250\n",
      "\n",
      "Epoch : 76 , Training loss : 740869.8125\n",
      "\n",
      "Epoch : 77 , Training loss : 730386.0000\n",
      "\n",
      "Epoch : 78 , Training loss : 720188.3125\n",
      "\n",
      "Epoch : 79 , Training loss : 710264.8750\n",
      "\n",
      "Epoch : 80 , Training loss : 700586.3750\n",
      "\n",
      "Epoch : 81 , Training loss : 691157.5625\n",
      "\n",
      "Epoch : 82 , Training loss : 681971.9375\n",
      "\n",
      "Epoch : 83 , Training loss : 672986.1875\n",
      "\n",
      "Epoch : 84 , Training loss : 664229.3750\n",
      "\n",
      "Epoch : 85 , Training loss : 655673.6875\n",
      "\n",
      "Epoch : 86 , Training loss : 647333.3125\n",
      "\n",
      "Epoch : 87 , Training loss : 639173.7500\n",
      "\n",
      "Epoch : 88 , Training loss : 631193.6250\n",
      "\n",
      "Epoch : 89 , Training loss : 623413.0000\n",
      "\n",
      "Epoch : 90 , Training loss : 615797.3750\n",
      "\n",
      "Epoch : 91 , Training loss : 608347.3750\n",
      "\n",
      "Epoch : 92 , Training loss : 601037.5000\n",
      "\n",
      "Epoch : 93 , Training loss : 593894.4375\n",
      "\n",
      "Epoch : 94 , Training loss : 586897.4375\n",
      "\n",
      "Epoch : 95 , Training loss : 580041.0000\n",
      "\n",
      "Epoch : 96 , Training loss : 573343.1250\n",
      "\n",
      "Epoch : 97 , Training loss : 566761.5000\n",
      "\n",
      "Epoch : 98 , Training loss : 560314.3750\n",
      "\n",
      "Epoch : 99 , Training loss : 553990.1875\n",
      "\n",
      "Epoch : 100 , Training loss : 547789.2500\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.925708\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  6\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 6\n",
      "Epoch : 1 , Training loss : 11176917.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 10600072.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 10049721.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 9525031.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 9024919.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6 , Training loss : 8549072.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 8096836.5000\n",
      "\n",
      "Epoch : 8 , Training loss : 7667460.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 7260292.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 6874511.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 6509334.0000\n",
      "\n",
      "Epoch : 12 , Training loss : 6163870.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 5837289.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 5528854.5000\n",
      "\n",
      "Epoch : 15 , Training loss : 5237817.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 4963263.5000\n",
      "\n",
      "Epoch : 17 , Training loss : 4704309.5000\n",
      "\n",
      "Epoch : 18 , Training loss : 4460220.0000\n",
      "\n",
      "Epoch : 19 , Training loss : 4230236.0000\n",
      "\n",
      "Epoch : 20 , Training loss : 4013575.0000\n",
      "\n",
      "Epoch : 21 , Training loss : 3809524.2500\n",
      "\n",
      "Epoch : 22 , Training loss : 3617468.2500\n",
      "\n",
      "Epoch : 23 , Training loss : 3436830.2500\n",
      "\n",
      "Epoch : 24 , Training loss : 3266817.0000\n",
      "\n",
      "Epoch : 25 , Training loss : 3106795.0000\n",
      "\n",
      "Epoch : 26 , Training loss : 2956083.5000\n",
      "\n",
      "Epoch : 27 , Training loss : 2814185.7500\n",
      "\n",
      "Epoch : 28 , Training loss : 2680577.5000\n",
      "\n",
      "Epoch : 29 , Training loss : 2554763.0000\n",
      "\n",
      "Epoch : 30 , Training loss : 2436287.0000\n",
      "\n",
      "Epoch : 31 , Training loss : 2324667.0000\n",
      "\n",
      "Epoch : 32 , Training loss : 2219498.2500\n",
      "\n",
      "Epoch : 33 , Training loss : 2120361.5000\n",
      "\n",
      "Epoch : 34 , Training loss : 2026912.1250\n",
      "\n",
      "Epoch : 35 , Training loss : 1938800.6250\n",
      "\n",
      "Epoch : 36 , Training loss : 1855675.8750\n",
      "\n",
      "Epoch : 37 , Training loss : 1777257.2500\n",
      "\n",
      "Epoch : 38 , Training loss : 1703293.2500\n",
      "\n",
      "Epoch : 39 , Training loss : 1633544.7500\n",
      "\n",
      "Epoch : 40 , Training loss : 1567809.6250\n",
      "\n",
      "Epoch : 41 , Training loss : 1505789.5000\n",
      "\n",
      "Epoch : 42 , Training loss : 1447218.6250\n",
      "\n",
      "Epoch : 43 , Training loss : 1391861.3750\n",
      "\n",
      "Epoch : 44 , Training loss : 1339477.6250\n",
      "\n",
      "Epoch : 45 , Training loss : 1289839.5000\n",
      "\n",
      "Epoch : 46 , Training loss : 1242804.6250\n",
      "\n",
      "Epoch : 47 , Training loss : 1198190.8750\n",
      "\n",
      "Epoch : 48 , Training loss : 1155842.1250\n",
      "\n",
      "Epoch : 49 , Training loss : 1115627.3750\n",
      "\n",
      "Epoch : 50 , Training loss : 1077416.6250\n",
      "\n",
      "Epoch : 51 , Training loss : 1041086.9375\n",
      "\n",
      "Epoch : 52 , Training loss : 1006504.3125\n",
      "\n",
      "Epoch : 53 , Training loss : 973586.8750\n",
      "\n",
      "Epoch : 54 , Training loss : 942217.2500\n",
      "\n",
      "Epoch : 55 , Training loss : 912303.8125\n",
      "\n",
      "Epoch : 56 , Training loss : 883772.8750\n",
      "\n",
      "Epoch : 57 , Training loss : 856563.6875\n",
      "\n",
      "Epoch : 58 , Training loss : 830558.5000\n",
      "\n",
      "Epoch : 59 , Training loss : 805732.3750\n",
      "\n",
      "Epoch : 60 , Training loss : 781997.5625\n",
      "\n",
      "Epoch : 61 , Training loss : 759309.7500\n",
      "\n",
      "Epoch : 62 , Training loss : 737603.6250\n",
      "\n",
      "Epoch : 63 , Training loss : 716807.8750\n",
      "\n",
      "Epoch : 64 , Training loss : 696881.3750\n",
      "\n",
      "Epoch : 65 , Training loss : 677790.0625\n",
      "\n",
      "Epoch : 66 , Training loss : 659486.3750\n",
      "\n",
      "Epoch : 67 , Training loss : 641929.8125\n",
      "\n",
      "Epoch : 68 , Training loss : 625086.7500\n",
      "\n",
      "Epoch : 69 , Training loss : 608918.7500\n",
      "\n",
      "Epoch : 70 , Training loss : 593384.0000\n",
      "\n",
      "Epoch : 71 , Training loss : 578470.1250\n",
      "\n",
      "Epoch : 72 , Training loss : 564138.7500\n",
      "\n",
      "Epoch : 73 , Training loss : 550344.7500\n",
      "\n",
      "Epoch : 74 , Training loss : 537077.4375\n",
      "\n",
      "Epoch : 75 , Training loss : 524308.1250\n",
      "\n",
      "Epoch : 76 , Training loss : 512000.1875\n",
      "\n",
      "Epoch : 77 , Training loss : 500138.7500\n",
      "\n",
      "Epoch : 78 , Training loss : 488698.7812\n",
      "\n",
      "Epoch : 79 , Training loss : 477676.6250\n",
      "\n",
      "Epoch : 80 , Training loss : 467057.5625\n",
      "\n",
      "Epoch : 81 , Training loss : 456811.5938\n",
      "\n",
      "Epoch : 82 , Training loss : 446917.0000\n",
      "\n",
      "Epoch : 83 , Training loss : 437382.2500\n",
      "\n",
      "Epoch : 84 , Training loss : 428162.7188\n",
      "\n",
      "Epoch : 85 , Training loss : 419264.6562\n",
      "\n",
      "Epoch : 86 , Training loss : 410677.5312\n",
      "\n",
      "Epoch : 87 , Training loss : 402407.0000\n",
      "\n",
      "Epoch : 88 , Training loss : 394417.4375\n",
      "\n",
      "Epoch : 89 , Training loss : 386696.5312\n",
      "\n",
      "Epoch : 90 , Training loss : 379231.2188\n",
      "\n",
      "Epoch : 91 , Training loss : 371997.3438\n",
      "\n",
      "Epoch : 92 , Training loss : 365012.6875\n",
      "\n",
      "Epoch : 93 , Training loss : 358249.9375\n",
      "\n",
      "Epoch : 94 , Training loss : 351700.2812\n",
      "\n",
      "Epoch : 95 , Training loss : 345360.3125\n",
      "\n",
      "Epoch : 96 , Training loss : 339228.7500\n",
      "\n",
      "Epoch : 97 , Training loss : 333265.4062\n",
      "\n",
      "Epoch : 98 , Training loss : 327496.7500\n",
      "\n",
      "Epoch : 99 , Training loss : 321916.7500\n",
      "\n",
      "Epoch : 100 , Training loss : 316512.6250\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.736526\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  7\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 7\n",
      "Epoch : 1 , Training loss : 10537237.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 9980663.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 9453646.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 8955826.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 8485824.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 8042125.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 7623614.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 7229289.5000\n",
      "\n",
      "Epoch : 9 , Training loss : 6858005.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 6508642.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 6179924.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 5870762.5000\n",
      "\n",
      "Epoch : 13 , Training loss : 5580000.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 5306691.0000\n",
      "\n",
      "Epoch : 15 , Training loss : 5049676.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 4807856.0000\n",
      "\n",
      "Epoch : 17 , Training loss : 4580322.0000\n",
      "\n",
      "Epoch : 18 , Training loss : 4366271.5000\n",
      "\n",
      "Epoch : 19 , Training loss : 4164772.2500\n",
      "\n",
      "Epoch : 20 , Training loss : 3975057.7500\n",
      "\n",
      "Epoch : 21 , Training loss : 3796382.0000\n",
      "\n",
      "Epoch : 22 , Training loss : 3628094.2500\n",
      "\n",
      "Epoch : 23 , Training loss : 3469527.5000\n",
      "\n",
      "Epoch : 24 , Training loss : 3320212.5000\n",
      "\n",
      "Epoch : 25 , Training loss : 3179363.0000\n",
      "\n",
      "Epoch : 26 , Training loss : 3046630.7500\n",
      "\n",
      "Epoch : 27 , Training loss : 2921541.7500\n",
      "\n",
      "Epoch : 28 , Training loss : 2803719.7500\n",
      "\n",
      "Epoch : 29 , Training loss : 2692637.2500\n",
      "\n",
      "Epoch : 30 , Training loss : 2587923.7500\n",
      "\n",
      "Epoch : 31 , Training loss : 2489291.2500\n",
      "\n",
      "Epoch : 32 , Training loss : 2396341.7500\n",
      "\n",
      "Epoch : 33 , Training loss : 2308734.5000\n",
      "\n",
      "Epoch : 34 , Training loss : 2226202.5000\n",
      "\n",
      "Epoch : 35 , Training loss : 2148397.5000\n",
      "\n",
      "Epoch : 36 , Training loss : 2075073.3750\n",
      "\n",
      "Epoch : 37 , Training loss : 2005948.3750\n",
      "\n",
      "Epoch : 38 , Training loss : 1940692.3750\n",
      "\n",
      "Epoch : 39 , Training loss : 1879149.0000\n",
      "\n",
      "Epoch : 40 , Training loss : 1821060.7500\n",
      "\n",
      "Epoch : 41 , Training loss : 1766216.0000\n",
      "\n",
      "Epoch : 42 , Training loss : 1714397.8750\n",
      "\n",
      "Epoch : 43 , Training loss : 1665404.3750\n",
      "\n",
      "Epoch : 44 , Training loss : 1619086.1250\n",
      "\n",
      "Epoch : 45 , Training loss : 1575245.1250\n",
      "\n",
      "Epoch : 46 , Training loss : 1533764.3750\n",
      "\n",
      "Epoch : 47 , Training loss : 1494562.8750\n",
      "\n",
      "Epoch : 48 , Training loss : 1457520.8750\n",
      "\n",
      "Epoch : 49 , Training loss : 1422376.0000\n",
      "\n",
      "Epoch : 50 , Training loss : 1388966.3750\n",
      "\n",
      "Epoch : 51 , Training loss : 1357162.1250\n",
      "\n",
      "Epoch : 52 , Training loss : 1326855.6250\n",
      "\n",
      "Epoch : 53 , Training loss : 1297976.1250\n",
      "\n",
      "Epoch : 54 , Training loss : 1270369.6250\n",
      "\n",
      "Epoch : 55 , Training loss : 1243954.2500\n",
      "\n",
      "Epoch : 56 , Training loss : 1218609.6250\n",
      "\n",
      "Epoch : 57 , Training loss : 1194315.3750\n",
      "\n",
      "Epoch : 58 , Training loss : 1170994.5000\n",
      "\n",
      "Epoch : 59 , Training loss : 1148544.6250\n",
      "\n",
      "Epoch : 60 , Training loss : 1126987.5000\n",
      "\n",
      "Epoch : 61 , Training loss : 1106218.5000\n",
      "\n",
      "Epoch : 62 , Training loss : 1086171.6250\n",
      "\n",
      "Epoch : 63 , Training loss : 1066819.6250\n",
      "\n",
      "Epoch : 64 , Training loss : 1048118.7500\n",
      "\n",
      "Epoch : 65 , Training loss : 1030033.5000\n",
      "\n",
      "Epoch : 66 , Training loss : 1012545.1250\n",
      "\n",
      "Epoch : 67 , Training loss : 995552.7500\n",
      "\n",
      "Epoch : 68 , Training loss : 979099.5625\n",
      "\n",
      "Epoch : 69 , Training loss : 963104.6250\n",
      "\n",
      "Epoch : 70 , Training loss : 947584.3750\n",
      "\n",
      "Epoch : 71 , Training loss : 932496.0000\n",
      "\n",
      "Epoch : 72 , Training loss : 917821.3750\n",
      "\n",
      "Epoch : 73 , Training loss : 903548.3750\n",
      "\n",
      "Epoch : 74 , Training loss : 889654.3125\n",
      "\n",
      "Epoch : 75 , Training loss : 876121.0000\n",
      "\n",
      "Epoch : 76 , Training loss : 862913.0625\n",
      "\n",
      "Epoch : 77 , Training loss : 850024.6250\n",
      "\n",
      "Epoch : 78 , Training loss : 837449.2500\n",
      "\n",
      "Epoch : 79 , Training loss : 825167.3750\n",
      "\n",
      "Epoch : 80 , Training loss : 813182.2500\n",
      "\n",
      "Epoch : 81 , Training loss : 801482.1250\n",
      "\n",
      "Epoch : 82 , Training loss : 790052.3750\n",
      "\n",
      "Epoch : 83 , Training loss : 778895.0000\n",
      "\n",
      "Epoch : 84 , Training loss : 767982.8750\n",
      "\n",
      "Epoch : 85 , Training loss : 757321.5625\n",
      "\n",
      "Epoch : 86 , Training loss : 746876.5000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 87 , Training loss : 736678.6250\n",
      "\n",
      "Epoch : 88 , Training loss : 726703.8125\n",
      "\n",
      "Epoch : 89 , Training loss : 716930.8125\n",
      "\n",
      "Epoch : 90 , Training loss : 707387.2500\n",
      "\n",
      "Epoch : 91 , Training loss : 698014.9375\n",
      "\n",
      "Epoch : 92 , Training loss : 688840.8750\n",
      "\n",
      "Epoch : 93 , Training loss : 679850.0000\n",
      "\n",
      "Epoch : 94 , Training loss : 671024.0625\n",
      "\n",
      "Epoch : 95 , Training loss : 662385.5000\n",
      "\n",
      "Epoch : 96 , Training loss : 653906.4375\n",
      "\n",
      "Epoch : 97 , Training loss : 645592.3750\n",
      "\n",
      "Epoch : 98 , Training loss : 637444.1250\n",
      "\n",
      "Epoch : 99 , Training loss : 629437.1875\n",
      "\n",
      "Epoch : 100 , Training loss : 621550.8750\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.723663\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  8\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 8\n",
      "Epoch : 1 , Training loss : 5213355.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 4932938.5000\n",
      "\n",
      "Epoch : 3 , Training loss : 4666672.5000\n",
      "\n",
      "Epoch : 4 , Training loss : 4414201.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 4175194.2500\n",
      "\n",
      "Epoch : 6 , Training loss : 3949237.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 3736004.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 3535068.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 3345951.2500\n",
      "\n",
      "Epoch : 10 , Training loss : 3168129.0000\n",
      "\n",
      "Epoch : 11 , Training loss : 3001099.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 2844212.7500\n",
      "\n",
      "Epoch : 13 , Training loss : 2696512.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 2556714.5000\n",
      "\n",
      "Epoch : 15 , Training loss : 2424381.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 2299863.5000\n",
      "\n",
      "Epoch : 17 , Training loss : 2182816.2500\n",
      "\n",
      "Epoch : 18 , Training loss : 2072795.3750\n",
      "\n",
      "Epoch : 19 , Training loss : 1969352.3750\n",
      "\n",
      "Epoch : 20 , Training loss : 1871729.8750\n",
      "\n",
      "Epoch : 21 , Training loss : 1779825.7500\n",
      "\n",
      "Epoch : 22 , Training loss : 1693254.5000\n",
      "\n",
      "Epoch : 23 , Training loss : 1611682.6250\n",
      "\n",
      "Epoch : 24 , Training loss : 1534826.3750\n",
      "\n",
      "Epoch : 25 , Training loss : 1462458.0000\n",
      "\n",
      "Epoch : 26 , Training loss : 1394286.8750\n",
      "\n",
      "Epoch : 27 , Training loss : 1330047.6250\n",
      "\n",
      "Epoch : 28 , Training loss : 1269206.0000\n",
      "\n",
      "Epoch : 29 , Training loss : 1211813.8750\n",
      "\n",
      "Epoch : 30 , Training loss : 1157694.1250\n",
      "\n",
      "Epoch : 31 , Training loss : 1106612.8750\n",
      "\n",
      "Epoch : 32 , Training loss : 1058410.3750\n",
      "\n",
      "Epoch : 33 , Training loss : 1012883.0000\n",
      "\n",
      "Epoch : 34 , Training loss : 969854.0625\n",
      "\n",
      "Epoch : 35 , Training loss : 929167.6250\n",
      "\n",
      "Epoch : 36 , Training loss : 890701.5625\n",
      "\n",
      "Epoch : 37 , Training loss : 854384.2500\n",
      "\n",
      "Epoch : 38 , Training loss : 820042.6250\n",
      "\n",
      "Epoch : 39 , Training loss : 787538.0625\n",
      "\n",
      "Epoch : 40 , Training loss : 756753.6250\n",
      "\n",
      "Epoch : 41 , Training loss : 727603.1875\n",
      "\n",
      "Epoch : 42 , Training loss : 699964.2500\n",
      "\n",
      "Epoch : 43 , Training loss : 673769.6250\n",
      "\n",
      "Epoch : 44 , Training loss : 648871.6250\n",
      "\n",
      "Epoch : 45 , Training loss : 625201.4375\n",
      "\n",
      "Epoch : 46 , Training loss : 602661.3125\n",
      "\n",
      "Epoch : 47 , Training loss : 581220.8750\n",
      "\n",
      "Epoch : 48 , Training loss : 560767.0625\n",
      "\n",
      "Epoch : 49 , Training loss : 541217.9375\n",
      "\n",
      "Epoch : 50 , Training loss : 522533.5625\n",
      "\n",
      "Epoch : 51 , Training loss : 504694.5000\n",
      "\n",
      "Epoch : 52 , Training loss : 487663.2500\n",
      "\n",
      "Epoch : 53 , Training loss : 471408.0000\n",
      "\n",
      "Epoch : 54 , Training loss : 455894.6875\n",
      "\n",
      "Epoch : 55 , Training loss : 441094.1562\n",
      "\n",
      "Epoch : 56 , Training loss : 426977.1875\n",
      "\n",
      "Epoch : 57 , Training loss : 413528.3125\n",
      "\n",
      "Epoch : 58 , Training loss : 400677.8125\n",
      "\n",
      "Epoch : 59 , Training loss : 388404.6250\n",
      "\n",
      "Epoch : 60 , Training loss : 376676.8125\n",
      "\n",
      "Epoch : 61 , Training loss : 365468.5625\n",
      "\n",
      "Epoch : 62 , Training loss : 354748.3438\n",
      "\n",
      "Epoch : 63 , Training loss : 344510.3438\n",
      "\n",
      "Epoch : 64 , Training loss : 334717.2188\n",
      "\n",
      "Epoch : 65 , Training loss : 325356.1250\n",
      "\n",
      "Epoch : 66 , Training loss : 316405.9688\n",
      "\n",
      "Epoch : 67 , Training loss : 307851.5938\n",
      "\n",
      "Epoch : 68 , Training loss : 299667.8750\n",
      "\n",
      "Epoch : 69 , Training loss : 291830.1562\n",
      "\n",
      "Epoch : 70 , Training loss : 284333.4375\n",
      "\n",
      "Epoch : 71 , Training loss : 277156.3750\n",
      "\n",
      "Epoch : 72 , Training loss : 270270.2188\n",
      "\n",
      "Epoch : 73 , Training loss : 263662.6250\n",
      "\n",
      "Epoch : 74 , Training loss : 257322.7188\n",
      "\n",
      "Epoch : 75 , Training loss : 251243.7812\n",
      "\n",
      "Epoch : 76 , Training loss : 245391.4062\n",
      "\n",
      "Epoch : 77 , Training loss : 239759.9375\n",
      "\n",
      "Epoch : 78 , Training loss : 234344.2969\n",
      "\n",
      "Epoch : 79 , Training loss : 229130.4375\n",
      "\n",
      "Epoch : 80 , Training loss : 224106.4062\n",
      "\n",
      "Epoch : 81 , Training loss : 219249.9375\n",
      "\n",
      "Epoch : 82 , Training loss : 214568.5781\n",
      "\n",
      "Epoch : 83 , Training loss : 210056.7500\n",
      "\n",
      "Epoch : 84 , Training loss : 205703.1250\n",
      "\n",
      "Epoch : 85 , Training loss : 201509.5312\n",
      "\n",
      "Epoch : 86 , Training loss : 197457.5938\n",
      "\n",
      "Epoch : 87 , Training loss : 193535.2812\n",
      "\n",
      "Epoch : 88 , Training loss : 189741.6875\n",
      "\n",
      "Epoch : 89 , Training loss : 186075.2500\n",
      "\n",
      "Epoch : 90 , Training loss : 182519.6562\n",
      "\n",
      "Epoch : 91 , Training loss : 179074.4688\n",
      "\n",
      "Epoch : 92 , Training loss : 175739.8438\n",
      "\n",
      "Epoch : 93 , Training loss : 172505.1875\n",
      "\n",
      "Epoch : 94 , Training loss : 169364.1094\n",
      "\n",
      "Epoch : 95 , Training loss : 166320.7344\n",
      "\n",
      "Epoch : 96 , Training loss : 163371.3750\n",
      "\n",
      "Epoch : 97 , Training loss : 160515.8438\n",
      "\n",
      "Epoch : 98 , Training loss : 157734.7969\n",
      "\n",
      "Epoch : 99 , Training loss : 155030.7500\n",
      "\n",
      "Epoch : 100 , Training loss : 152411.4375\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.900836\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  9\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 9\n",
      "Epoch : 1 , Training loss : 11372058.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 10790196.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 10234506.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 9704685.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 9200789.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 8722269.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 8268380.5000\n",
      "\n",
      "Epoch : 8 , Training loss : 7838239.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 7430706.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 7044738.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 6679564.0000\n",
      "\n",
      "Epoch : 12 , Training loss : 6334353.5000\n",
      "\n",
      "Epoch : 13 , Training loss : 6008263.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 5700426.0000\n",
      "\n",
      "Epoch : 15 , Training loss : 5410010.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 5136219.0000\n",
      "\n",
      "Epoch : 17 , Training loss : 4878186.5000\n",
      "\n",
      "Epoch : 18 , Training loss : 4635102.5000\n",
      "\n",
      "Epoch : 19 , Training loss : 4406132.5000\n",
      "\n",
      "Epoch : 20 , Training loss : 4190518.5000\n",
      "\n",
      "Epoch : 21 , Training loss : 3987411.2500\n",
      "\n",
      "Epoch : 22 , Training loss : 3795993.0000\n",
      "\n",
      "Epoch : 23 , Training loss : 3615618.5000\n",
      "\n",
      "Epoch : 24 , Training loss : 3445627.0000\n",
      "\n",
      "Epoch : 25 , Training loss : 3285427.7500\n",
      "\n",
      "Epoch : 26 , Training loss : 3134591.0000\n",
      "\n",
      "Epoch : 27 , Training loss : 2992473.7500\n",
      "\n",
      "Epoch : 28 , Training loss : 2858621.7500\n",
      "\n",
      "Epoch : 29 , Training loss : 2732596.0000\n",
      "\n",
      "Epoch : 30 , Training loss : 2613928.0000\n",
      "\n",
      "Epoch : 31 , Training loss : 2502109.0000\n",
      "\n",
      "Epoch : 32 , Training loss : 2396776.7500\n",
      "\n",
      "Epoch : 33 , Training loss : 2297506.2500\n",
      "\n",
      "Epoch : 34 , Training loss : 2203887.5000\n",
      "\n",
      "Epoch : 35 , Training loss : 2115623.5000\n",
      "\n",
      "Epoch : 36 , Training loss : 2032349.8750\n",
      "\n",
      "Epoch : 37 , Training loss : 1953759.6250\n",
      "\n",
      "Epoch : 38 , Training loss : 1879558.1250\n",
      "\n",
      "Epoch : 39 , Training loss : 1809485.7500\n",
      "\n",
      "Epoch : 40 , Training loss : 1743285.5000\n",
      "\n",
      "Epoch : 41 , Training loss : 1680708.7500\n",
      "\n",
      "Epoch : 42 , Training loss : 1621524.0000\n",
      "\n",
      "Epoch : 43 , Training loss : 1565591.5000\n",
      "\n",
      "Epoch : 44 , Training loss : 1512637.0000\n",
      "\n",
      "Epoch : 45 , Training loss : 1462525.3750\n",
      "\n",
      "Epoch : 46 , Training loss : 1415085.0000\n",
      "\n",
      "Epoch : 47 , Training loss : 1370175.3750\n",
      "\n",
      "Epoch : 48 , Training loss : 1327619.2500\n",
      "\n",
      "Epoch : 49 , Training loss : 1287294.0000\n",
      "\n",
      "Epoch : 50 , Training loss : 1249048.7500\n",
      "\n",
      "Epoch : 51 , Training loss : 1212795.7500\n",
      "\n",
      "Epoch : 52 , Training loss : 1178398.0000\n",
      "\n",
      "Epoch : 53 , Training loss : 1145744.0000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 54 , Training loss : 1114724.0000\n",
      "\n",
      "Epoch : 55 , Training loss : 1085255.2500\n",
      "\n",
      "Epoch : 56 , Training loss : 1057253.0000\n",
      "\n",
      "Epoch : 57 , Training loss : 1030616.0000\n",
      "\n",
      "Epoch : 58 , Training loss : 1005287.7500\n",
      "\n",
      "Epoch : 59 , Training loss : 981155.5000\n",
      "\n",
      "Epoch : 60 , Training loss : 958179.3125\n",
      "\n",
      "Epoch : 61 , Training loss : 936282.3750\n",
      "\n",
      "Epoch : 62 , Training loss : 915395.0000\n",
      "\n",
      "Epoch : 63 , Training loss : 895455.0000\n",
      "\n",
      "Epoch : 64 , Training loss : 876441.6250\n",
      "\n",
      "Epoch : 65 , Training loss : 858256.8750\n",
      "\n",
      "Epoch : 66 , Training loss : 840891.0625\n",
      "\n",
      "Epoch : 67 , Training loss : 824273.8750\n",
      "\n",
      "Epoch : 68 , Training loss : 808377.1875\n",
      "\n",
      "Epoch : 69 , Training loss : 793145.1250\n",
      "\n",
      "Epoch : 70 , Training loss : 778547.1250\n",
      "\n",
      "Epoch : 71 , Training loss : 764558.9375\n",
      "\n",
      "Epoch : 72 , Training loss : 751139.9375\n",
      "\n",
      "Epoch : 73 , Training loss : 738251.1250\n",
      "\n",
      "Epoch : 74 , Training loss : 725861.0625\n",
      "\n",
      "Epoch : 75 , Training loss : 713961.8750\n",
      "\n",
      "Epoch : 76 , Training loss : 702525.6250\n",
      "\n",
      "Epoch : 77 , Training loss : 691504.5000\n",
      "\n",
      "Epoch : 78 , Training loss : 680893.8750\n",
      "\n",
      "Epoch : 79 , Training loss : 670657.5625\n",
      "\n",
      "Epoch : 80 , Training loss : 660788.9375\n",
      "\n",
      "Epoch : 81 , Training loss : 651275.1250\n",
      "\n",
      "Epoch : 82 , Training loss : 642075.5000\n",
      "\n",
      "Epoch : 83 , Training loss : 633202.0625\n",
      "\n",
      "Epoch : 84 , Training loss : 624605.5000\n",
      "\n",
      "Epoch : 85 , Training loss : 616277.0000\n",
      "\n",
      "Epoch : 86 , Training loss : 608228.5000\n",
      "\n",
      "Epoch : 87 , Training loss : 600414.6875\n",
      "\n",
      "Epoch : 88 , Training loss : 592841.0000\n",
      "\n",
      "Epoch : 89 , Training loss : 585499.4375\n",
      "\n",
      "Epoch : 90 , Training loss : 578373.1875\n",
      "\n",
      "Epoch : 91 , Training loss : 571445.8750\n",
      "\n",
      "Epoch : 92 , Training loss : 564713.6250\n",
      "\n",
      "Epoch : 93 , Training loss : 558163.7500\n",
      "\n",
      "Epoch : 94 , Training loss : 551793.2500\n",
      "\n",
      "Epoch : 95 , Training loss : 545597.8750\n",
      "\n",
      "Epoch : 96 , Training loss : 539558.3125\n",
      "\n",
      "Epoch : 97 , Training loss : 533661.8750\n",
      "\n",
      "Epoch : 98 , Training loss : 527935.1875\n",
      "\n",
      "Epoch : 99 , Training loss : 522339.7188\n",
      "\n",
      "Epoch : 100 , Training loss : 516876.0938\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.723824\n",
      "Training for stock : ABIRLANUVO\n",
      "seed :  10\n",
      "starting at time: 1571845163.8744543\n",
      "****************************************************************************************************\n",
      "seed: 10\n",
      "Epoch : 1 , Training loss : 12134130.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 11565097.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 11018613.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 10495006.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 9994087.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 9515420.0000\n",
      "\n",
      "Epoch : 7 , Training loss : 9058390.0000\n",
      "\n",
      "Epoch : 8 , Training loss : 8622443.0000\n",
      "\n",
      "Epoch : 9 , Training loss : 8206985.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 7811395.5000\n",
      "\n",
      "Epoch : 11 , Training loss : 7435106.5000\n",
      "\n",
      "Epoch : 12 , Training loss : 7077465.0000\n",
      "\n",
      "Epoch : 13 , Training loss : 6738043.5000\n",
      "\n",
      "Epoch : 14 , Training loss : 6416158.5000\n",
      "\n",
      "Epoch : 15 , Training loss : 6111290.5000\n",
      "\n",
      "Epoch : 16 , Training loss : 5822656.0000\n",
      "\n",
      "Epoch : 17 , Training loss : 5549672.5000\n",
      "\n",
      "Epoch : 18 , Training loss : 5291520.5000\n",
      "\n",
      "Epoch : 19 , Training loss : 5047627.0000\n",
      "\n",
      "Epoch : 20 , Training loss : 4817190.0000\n",
      "\n",
      "Epoch : 21 , Training loss : 4599713.0000\n",
      "\n",
      "Epoch : 22 , Training loss : 4394345.0000\n",
      "\n",
      "Epoch : 23 , Training loss : 4200496.5000\n",
      "\n",
      "Epoch : 24 , Training loss : 4017505.5000\n",
      "\n",
      "Epoch : 25 , Training loss : 3844656.2500\n",
      "\n",
      "Epoch : 26 , Training loss : 3681404.0000\n",
      "\n",
      "Epoch : 27 , Training loss : 3527197.2500\n",
      "\n",
      "Epoch : 28 , Training loss : 3381534.2500\n",
      "\n",
      "Epoch : 29 , Training loss : 3243945.0000\n",
      "\n",
      "Epoch : 30 , Training loss : 3113875.7500\n",
      "\n",
      "Epoch : 31 , Training loss : 2990897.5000\n",
      "\n",
      "Epoch : 32 , Training loss : 2874622.7500\n",
      "\n",
      "Epoch : 33 , Training loss : 2764595.0000\n",
      "\n",
      "Epoch : 34 , Training loss : 2660460.2500\n",
      "\n",
      "Epoch : 35 , Training loss : 2561832.0000\n",
      "\n",
      "Epoch : 36 , Training loss : 2468415.7500\n",
      "\n",
      "Epoch : 37 , Training loss : 2379861.5000\n",
      "\n",
      "Epoch : 38 , Training loss : 2295889.5000\n",
      "\n",
      "Epoch : 39 , Training loss : 2216191.2500\n",
      "\n",
      "Epoch : 40 , Training loss : 2140536.7500\n",
      "\n",
      "Epoch : 41 , Training loss : 2068697.6250\n",
      "\n",
      "Epoch : 42 , Training loss : 2000460.2500\n",
      "\n",
      "Epoch : 43 , Training loss : 1935553.2500\n",
      "\n",
      "Epoch : 44 , Training loss : 1873884.6250\n",
      "\n",
      "Epoch : 45 , Training loss : 1815216.0000\n",
      "\n",
      "Epoch : 46 , Training loss : 1759391.2500\n",
      "\n",
      "Epoch : 47 , Training loss : 1706220.1250\n",
      "\n",
      "Epoch : 48 , Training loss : 1655606.3750\n",
      "\n",
      "Epoch : 49 , Training loss : 1607370.1250\n",
      "\n",
      "Epoch : 50 , Training loss : 1561419.8750\n",
      "\n",
      "Epoch : 51 , Training loss : 1517611.2500\n",
      "\n",
      "Epoch : 52 , Training loss : 1475856.0000\n",
      "\n",
      "Epoch : 53 , Training loss : 1436012.2500\n",
      "\n",
      "Epoch : 54 , Training loss : 1397976.1250\n",
      "\n",
      "Epoch : 55 , Training loss : 1361678.8750\n",
      "\n",
      "Epoch : 56 , Training loss : 1327035.7500\n",
      "\n",
      "Epoch : 57 , Training loss : 1293920.2500\n",
      "\n",
      "Epoch : 58 , Training loss : 1262299.3750\n",
      "\n",
      "Epoch : 59 , Training loss : 1232066.2500\n",
      "\n",
      "Epoch : 60 , Training loss : 1203177.7500\n",
      "\n",
      "Epoch : 61 , Training loss : 1175548.5000\n",
      "\n",
      "Epoch : 62 , Training loss : 1149120.8750\n",
      "\n",
      "Epoch : 63 , Training loss : 1123815.3750\n",
      "\n",
      "Epoch : 64 , Training loss : 1099600.8750\n",
      "\n",
      "Epoch : 65 , Training loss : 1076405.0000\n",
      "\n",
      "Epoch : 66 , Training loss : 1054194.7500\n",
      "\n",
      "Epoch : 67 , Training loss : 1032912.1250\n",
      "\n",
      "Epoch : 68 , Training loss : 1012478.8125\n",
      "\n",
      "Epoch : 69 , Training loss : 992895.7500\n",
      "\n",
      "Epoch : 70 , Training loss : 974096.5000\n",
      "\n",
      "Epoch : 71 , Training loss : 956044.2500\n",
      "\n",
      "Epoch : 72 , Training loss : 938717.0000\n",
      "\n",
      "Epoch : 73 , Training loss : 922044.6875\n",
      "\n",
      "Epoch : 74 , Training loss : 906008.7500\n",
      "\n",
      "Epoch : 75 , Training loss : 890587.6875\n",
      "\n",
      "Epoch : 76 , Training loss : 875749.1250\n",
      "\n",
      "Epoch : 77 , Training loss : 861446.6250\n",
      "\n",
      "Epoch : 78 , Training loss : 847656.5625\n",
      "\n",
      "Epoch : 79 , Training loss : 834367.1250\n",
      "\n",
      "Epoch : 80 , Training loss : 821529.5000\n",
      "\n",
      "Epoch : 81 , Training loss : 809155.6250\n",
      "\n",
      "Epoch : 82 , Training loss : 797199.9375\n",
      "\n",
      "Epoch : 83 , Training loss : 785635.5625\n",
      "\n",
      "Epoch : 84 , Training loss : 774449.3125\n",
      "\n",
      "Epoch : 85 , Training loss : 763639.9375\n",
      "\n",
      "Epoch : 86 , Training loss : 753162.3750\n",
      "\n",
      "Epoch : 87 , Training loss : 743008.3750\n",
      "\n",
      "Epoch : 88 , Training loss : 733165.5000\n",
      "\n",
      "Epoch : 89 , Training loss : 723624.1875\n",
      "\n",
      "Epoch : 90 , Training loss : 714353.5625\n",
      "\n",
      "Epoch : 91 , Training loss : 705332.6250\n",
      "\n",
      "Epoch : 92 , Training loss : 696562.1250\n",
      "\n",
      "Epoch : 93 , Training loss : 688039.1250\n",
      "\n",
      "Epoch : 94 , Training loss : 679746.3750\n",
      "\n",
      "Epoch : 95 , Training loss : 671656.7500\n",
      "\n",
      "Epoch : 96 , Training loss : 663793.6875\n",
      "\n",
      "Epoch : 97 , Training loss : 656123.0000\n",
      "\n",
      "Epoch : 98 , Training loss : 648662.8125\n",
      "\n",
      "Epoch : 99 , Training loss : 641378.1250\n",
      "\n",
      "Epoch : 100 , Training loss : 634283.0625\n",
      "\n",
      "****************************************************************************************************\n",
      "Shape of Z_train: (40, 684)\n",
      "Shape of Z_test:  (40, 171)\n",
      "****************************************************************************************************\n",
      "****************************************************************************************************\n",
      "\n",
      "\n",
      "time taken for training one stock: 0:00:05.740074\n",
      "time taken for one stock through all seeds 0:00:59.366644\n",
      "prev_day_values.shape: (229,)\n",
      "X_test.shape: (229, 5, 5)\n",
      "Y_test.shape: (229,)\n",
      "next_day_values.shape: (1145,)\n",
      "Training for stock : ACC\n",
      "seed :  1\n",
      "starting at time: 1571845223.2411566\n",
      "****************************************************************************************************\n",
      "seed: 1\n",
      "Epoch : 1 , Training loss : 10604472.0000\n",
      "\n",
      "Epoch : 2 , Training loss : 10055826.0000\n",
      "\n",
      "Epoch : 3 , Training loss : 9532251.0000\n",
      "\n",
      "Epoch : 4 , Training loss : 9033281.0000\n",
      "\n",
      "Epoch : 5 , Training loss : 8558216.0000\n",
      "\n",
      "Epoch : 6 , Training loss : 8106472.5000\n",
      "\n",
      "Epoch : 7 , Training loss : 7677390.5000\n",
      "\n",
      "Epoch : 8 , Training loss : 7270278.5000\n",
      "\n",
      "Epoch : 9 , Training loss : 6884409.0000\n",
      "\n",
      "Epoch : 10 , Training loss : 6519007.0000\n",
      "\n",
      "Epoch : 11 , Training loss : 6173362.0000\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-27a1c447f516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mZtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtr_loss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtr_loss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstock\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1f0d52bfb6b9>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(lr, epochs, momentum, X_train, Y_train, X_test, Y_test, batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1f0d52bfb6b9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epoch, model, optimizer, train_loader, batch_size, mu, eps, lam)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mepochs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mfinal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_0 = time.time()\n",
    "tr_loss_dict = {}\n",
    "#stocks_list = ['ABIRLANUVO','ACC','ADANIENT','ADANIPORTS','ADANIPOWER','AJANTPHARM', 'ALBK']#['ADANIPORTS']\n",
    "for stock in stocks_list[start:end]:\n",
    "    t0 = time.time()\n",
    "    _,windowed_data,_, next_day_values = getWindowedDataReg(data_df,stock,window_size)\n",
    "    feat_wise_data = getFeatWiseData(windowed_data,features_list)\n",
    "    feat_wise_data = feat_wise_data[:feat_wise_data.shape[0]-1]\n",
    "    prev_day_values = getPrevDayFeatures(feat_wise_data)\n",
    "    next_day_values = next_day_values[:,0]\n",
    "    next_day_values = next_day_values[0:next_day_values.shape[0]-1]\n",
    "    X_train,Y_train,X_test,Y_test = splitData(feat_wise_data,next_day_values,test_size=test_size)\n",
    "    #X_test = X_test[0:X_test.shape[0]-1]\n",
    "    prev_day_values = prev_day_values[X_train.shape[0]:][:,0]\n",
    "    #prev_day_values = prev_day_values[0:prev_day_values.shape[0]-1]\n",
    "    print('prev_day_values.shape:',prev_day_values.shape)\n",
    "    print('X_test.shape:',X_test.shape)\n",
    "    print('Y_test.shape:',Y_test.shape)\n",
    "    print('next_day_values.shape:', next_day_values.shape)\n",
    "    prev_val_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_yprev_cp.npy'\n",
    "    np.save(prev_val_path,prev_day_values)\n",
    "    for sd in range(1,seed_range+1):\n",
    "        t01 = time.time()\n",
    "        seed = sd\n",
    "        print('Training for stock :',stock)\n",
    "        print('seed : ',seed)\n",
    "        print('starting at time:',t0)\n",
    "        print('*'*100)\n",
    "        if custom_batch_size_flag:\n",
    "            batch_size = bs\n",
    "        else:\n",
    "            batch_size = X_train.shape[0]\n",
    "        Ztrain, Ztest,train_loss = train_on_batch(lr,epochs,momentum,X_train,Y_train,X_test,Y_test,batch_size)\n",
    "        tr_loss_dict[stock] = {}\n",
    "        tr_loss_dict[stock] = train_loss\n",
    "        train_loss = []\n",
    "        xtr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +'_' + str(test_size) + '_tl_xtrain' + str(seed) + '.npy'\n",
    "        ytr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +  '_' + str(test_size) + '_tl_ytrain' + str(seed) + '.npy'\n",
    "        xte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_xtest' + str(seed) + '.npy'\n",
    "        yte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_ytest' + str(seed) + '.npy'\n",
    "        np.save(xtr_path,Ztrain)\n",
    "        np.save(ytr_path,Y_train)\n",
    "        np.save(xte_path,Ztest)\n",
    "        np.save(yte_path,Y_test)\n",
    "        t11 = time.time()\n",
    "        print('*'*100)\n",
    "        #print('*'*100)\n",
    "        print('\\n')\n",
    "        print('time taken for training one stock:',datetime.timedelta(seconds = t11-t01))\n",
    "    t1 = time.time()\n",
    "    print('time taken for one stock through all seeds',datetime.timedelta(seconds = t1-t0))\n",
    "t_1 = time.time()\n",
    "print('time taken for 125 stocks through all seeds : ',str(datetime.timedelta(seconds = t_1-t_0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Regressor + Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regressor(Xtrain, Ytrain, Xtest, Ytest, alpha = 1.0, random_state = 1):\n",
    "    clf = Ridge(alpha=alpha,random_state = random_state)\n",
    "    clf.fit(Xtrain, Ytrain) \n",
    "    y_pred = clf.predict(Xtest)\n",
    "    mae = mean_absolute_error(Ytest, y_pred)\n",
    "    mse = mean_squared_error(Ytest, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    return y_pred, mae, mse,rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file_name = base_path+'Results2/Reg2/res_' + param_path + 'final.csv'\n",
    "pred_file_name = base_path+'Results2/Reg2/res_' + param_path + '_pred_global_final.csv'\n",
    "if os.path.exists(res_file_name):\n",
    "    os.remove(res_file_name)\n",
    "if os.path.exists(pred_file_name):\n",
    "    os.remove(pred_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock :  ABIRLANUVO\n",
      "seed :  1\n",
      "mae2 : 0.016774158058202034\n",
      "precision :0.5446, recall:0.6932, f1_score : 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooja/Desktop/paper/DeConFuse/codes/data_processing.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by = ['DATE'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days: 251\n",
      "stock :  ABIRLANUVO\n",
      "seed :  2\n",
      "mae2 : 0.022962603931086855\n",
      "precision :0.5474, recall:0.5909, f1_score : 0.5683\n",
      "stock :  ABIRLANUVO\n",
      "seed :  3\n",
      "mae2 : 0.020643607185934054\n",
      "precision :0.5242, recall:0.7386, f1_score : 0.6132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooja/Desktop/paper/DeConFuse/codes/data_processing.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by = ['DATE'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days: 251\n",
      "stock :  ABIRLANUVO\n",
      "seed :  4\n",
      "mae2 : 0.016827133810959165\n",
      "precision :0.5422, recall:0.5114, f1_score : 0.5263\n",
      "stock :  ABIRLANUVO\n",
      "seed :  5\n",
      "mae2 : 0.01581606129574119\n",
      "precision :0.4778, recall:0.4886, f1_score : 0.4831\n",
      "stock :  ABIRLANUVO\n",
      "seed :  6\n",
      "mae2 : 0.021776766351177333\n",
      "precision :0.5288, recall:0.6250, f1_score : 0.5729\n",
      "stock :  ABIRLANUVO\n",
      "seed :  7\n",
      "mae2 : 0.018084398954094198\n",
      "precision :0.5534, recall:0.6477, f1_score : 0.5969\n",
      "stock :  ABIRLANUVO\n",
      "seed :  8\n",
      "mae2 : 0.022936751294701005\n",
      "precision :0.5514, recall:0.6705, f1_score : 0.6051\n",
      "stock :  ABIRLANUVO\n",
      "seed :  9\n",
      "mae2 : 0.016069937478851104\n",
      "precision :0.5517, recall:0.5455, f1_score : 0.5486\n",
      "stock :  ABIRLANUVO\n",
      "seed :  10\n",
      "mae2 : 0.01917293870642169\n",
      "precision :0.4958, recall:0.6705, f1_score : 0.5700\n",
      "test_acc_dict data:\n",
      "        index  seed        mae      mae2          mse       rmse  \\\n",
      "0  ABIRLANUVO     3  30.262568  0.020644  1448.177533  38.054928   \n",
      "\n",
      "   Test Accuracy     AR  Precision    Recall  F1_score  \n",
      "0         52.047  47.05   0.524194  0.738636  0.613208  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Results2/Reg2/res__op1_4_op2_8_mp1_True_mp2_False_ks1_5_ks2_3_newfinal.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-08714aff6bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'column_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# else it exists so append without writing the header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3227\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         )\n\u001b[0;32m-> 3229\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# No explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Results2/Reg2/res__op1_4_op2_8_mp1_True_mp2_False_ks1_5_ks2_3_newfinal.csv'"
     ]
    }
   ],
   "source": [
    "gap = 5.0\n",
    "log_interval = 1\n",
    "cnt = 0 \n",
    "alpha = 0.1\n",
    "random_state = 1\n",
    "test_acc_dict  = {}\n",
    "final_results_df = pd.DataFrame()\n",
    "t0 = time.time()\n",
    "for stock in stocks_list[start:end]:\n",
    "    t01 = time.time()\n",
    "    temp_dict = {}\n",
    "    test_acc_dict[stock] = {}\n",
    "    best_ar = 0.0\n",
    "    best_mae = 100000.0\n",
    "    best_f1_score = 0.0\n",
    "    mae, mse, rmse = 0.0, 0.0, 0.0\n",
    "    found = 0\n",
    "    ytr_pred, yte_pred, tr_scores, te_score = [],[],[],[]\n",
    "    AR = 0\n",
    "    temp_ytr_pred, temp_yte_pred, temp_tr_scores, temp_te_scores = [],[],[],[]\n",
    "    temp_AR = 0\n",
    "    prev_val_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) +  '_tl_yprev_cp.npy'\n",
    "    prev_day_value = np.load(prev_val_path)\n",
    "    for sd in range(1,seed_range+1):\n",
    "        seed = sd\n",
    "        print('stock : ', stock)\n",
    "        xtr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +'_' + str(test_size) + '_tl_xtrain' + str(seed) + '.npy'\n",
    "        ytr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +  '_' + str(test_size) + '_tl_ytrain' + str(seed) + '.npy'\n",
    "        xte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_xtest' + str(seed) + '.npy'\n",
    "        yte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_ytest' + str(seed) + '.npy'\n",
    "        Ztrain = np.load(xtr_path)\n",
    "        Y_train = np.load(ytr_path)\n",
    "        Ztest = np.load(xte_path)\n",
    "        Y_test = np.load(yte_path)\n",
    "        print('seed : {: }'.format(seed))\n",
    "        yte_pred, mae, mse, rmse = ridge_regressor(Ztrain, Y_train, Ztest, Y_test, alpha = alpha, random_state = random_state)\n",
    "        pred_labels = np.where((yte_pred - prev_day_value)>0,1,0)\n",
    "        true_labels = np.where((Y_test - prev_day_value)>0,1,0)\n",
    "        te_acc = round(accuracy_score(true_labels, pred_labels)*100,3)\n",
    "        precision, recall, f1_score,_ = precision_recall_fscore_support(true_labels, pred_labels, pos_label=1, average='binary')\n",
    "        limit = Ztrain.shape[0]\n",
    "        #print(pred_labels)\n",
    "        mae2 = np.sum(np.abs(yte_pred-Y_test))/np.sum(Y_test)\n",
    "        print('mae2 :', mae2)\n",
    "        print('precision :{:.4f}, recall:{:.4f}, f1_score : {:.4f}'.format(precision, recall, f1_score))\n",
    "        if  f1_score > best_f1_score:\n",
    "            best_te_acc = te_acc\n",
    "            best_mae = mae\n",
    "            found = 1\n",
    "            best_f1_score = f1_score\n",
    "            test_acc_dict[stock]['seed'] = seed\n",
    "            test_acc_dict[stock]['mae'] = mae\n",
    "            test_acc_dict[stock]['mae2'] = mae2\n",
    "            test_acc_dict[stock]['mse'] = mse\n",
    "            test_acc_dict[stock]['rmse'] = rmse\n",
    "            test_acc_dict[stock]['Test Accuracy'] = te_acc\n",
    "            AR = compAnnualReturns(stock,pred_labels,data_df,window_size,limit)\n",
    "            test_acc_dict[stock]['AR'] = AR\n",
    "            test_acc_dict[stock]['Precision'] = precision\n",
    "            test_acc_dict[stock]['Recall'] = recall\n",
    "            test_acc_dict[stock]['F1_score'] = f1_score\n",
    "            temp_dict['pred_cp'] = yte_pred\n",
    "            temp_dict['Stock_Class'] = true_labels\n",
    "            temp_dict['ypred'] = pred_labels\n",
    "            temp_dict['found'] = found\n",
    "        \n",
    "    temp_final_df = pd.DataFrame(Y_test,columns=['actual_close_price'])\n",
    "    temp_final_df['predicted_close_price'] = temp_dict['pred_cp']\n",
    "    temp_final_df['prev_day_close_price'] = prev_day_value\n",
    "    temp_final_df['Stock_Class'] = temp_dict['Stock_Class']\n",
    "    temp_final_df['ypred'] = temp_dict['ypred']\n",
    "    temp_final_df['difference'] = temp_final_df['predicted_close_price'] - temp_final_df['actual_close_price']\n",
    "    temp_final_df['abs_difference'] = temp_final_df['difference'].abs()\n",
    "    temp_final_df['SYMBOL'] = stock\n",
    "    temp_final_df['found'] = found\n",
    "    final_results_df = pd.concat([final_results_df,temp_final_df],axis = 0)\n",
    "    cnt += 1\n",
    "    if cnt%log_interval==0:\n",
    "        df = pd.DataFrame.from_dict(data = test_acc_dict, orient = 'index').reset_index()\n",
    "        print('test_acc_dict data:')\n",
    "        print(df.head(2))\n",
    "        if not os.path.exists(res_file_name):\n",
    "            df.to_csv(res_file_name,index=None, header='column_names')\n",
    "        else: # else it exists so append without writing the header\n",
    "            df.to_csv(res_file_name, mode='a',index=None, header=False)\n",
    "        if not os.path.exists(pred_file_name):\n",
    "            final_results_df.to_csv(pred_file_name,index=None, header='column_names')\n",
    "        else: # else it exists so append without writing the header\n",
    "            final_results_df.to_csv(pred_file_name, mode='a',index=None, header=False)\n",
    "        print('final_results_df data:')\n",
    "        print(final_results_df.head(2))\n",
    "        test_acc_dict = {}\n",
    "        final_results_df = pd.DataFrame()\n",
    "    t11 = time.time()\n",
    "    print('time taken for one stock with ridge: ' ,datetime.timedelta(seconds = t11 - t01))\n",
    "    print('*'*100)\n",
    "t1 = time.time()\n",
    "print('time taken for all stocks with ridge: ' ,datetime.timedelta(seconds = t1 - t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res_file_name = base_path+'Results2/Reg2/Classification/convTL_TL_classification_2layers_results.csv'\n",
    "rf_pred_file_name = base_path+'Results2/Reg2/Classification/convTL_TL_classification_2layers_pred.csv'\n",
    "if os.path.exists(rf_res_file_name):\n",
    "     os.remove(rf_res_file_name)\n",
    "if os.path.exists(rf_pred_file_name):\n",
    "     os.remove(rf_pred_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clfRF(Ztrain,Y_train,Ztest,Y_test,n_clf=5,depth=1,rnd_state=11):\n",
    "    clf_rf = RandomForestClassifier(n_estimators=n_clf, max_depth=depth,random_state=rnd_state)\n",
    "    clf_rf.fit(Ztrain, Y_train)\n",
    "    ytr_rf_pred = clf_rf.predict(Ztrain)\n",
    "    yte_rf_pred = clf_rf.predict(Ztest)\n",
    "    tr_scores = clf_rf.predict_proba(Ztrain)\n",
    "    te_scores = clf_rf.predict_proba(Ztest)\n",
    "    return ytr_rf_pred, yte_rf_pred, tr_scores, te_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooja/Desktop/paper/DeConFuse/codes/data_processing.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by = ['DATE'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock :  ABIRLANUVO\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.6600985221674877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooja/Desktop/paper/DeConFuse/codes/data_processing.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by = ['DATE'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days: 251\n",
      "AR:  -0.41\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5759162303664922\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.5728155339805825\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5656565656565656\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.5376344086021506\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.576923076923077\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.5700483091787439\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5520833333333334\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.6339285714285714\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.5846153846153845\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.5560975609756097\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.582010582010582\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5619047619047619\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.57\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.6540284360189573\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5911330049261083\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.5252525252525252\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.5871559633027523\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5483870967741936\n",
      "seed :  1, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.6037735849056604\n",
      "stock :  ABIRLANUVO\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5520833333333334\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5217391304347826\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.6132075471698114\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5578947368421052\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6213592233009708\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5741626794258373\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.5251396648044693\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.6020408163265306\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.5684210526315789\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.5714285714285714\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.5773195876288659\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.5957446808510639\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5833333333333334\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5933014354066986\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.5771144278606964\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.508670520231214\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.6491228070175439\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.5825242718446602\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5729166666666667\n",
      "seed :  2, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.5876777251184835\n",
      "stock :  ABIRLANUVO\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5951219512195123\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5108695652173912\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.5628140703517588\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5812807881773399\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.5918367346938775\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5945945945945945\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.6069651741293532\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.601036269430052\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.5714285714285715\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.5414364640883979\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.5729729729729729\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.5786802030456852\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.6122448979591836\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5573770491803279\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.6009389671361502\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.6\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.5108695652173912\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.6169154228855721\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.6473429951690821\n",
      "seed :  3, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.5990338164251209\n",
      "stock :  ABIRLANUVO\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5756097560975609\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5699481865284974\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.6019417475728156\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5760869565217391\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6019417475728156\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5454545454545455\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.6037735849056604\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5490196078431373\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.6055045871559633\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.5756097560975609\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.6224489795918366\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.5961538461538463\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5297297297297298\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.6274509803921569\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.5943396226415094\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5384615384615384\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.5800000000000001\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.625\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.6030150753768844\n",
      "seed :  4, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.624390243902439\n",
      "stock :  ABIRLANUVO\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.635593220338983\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5656565656565656\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.5699481865284974\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5846153846153845\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6086956521739131\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5396825396825398\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.6048780487804879\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5786802030456852\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.56\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.47368421052631576\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.6086956521739131\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.6019417475728156\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5685279187817259\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5841584158415842\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.593607305936073\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5326633165829147\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.5911330049261083\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score :  0.5714285714285715\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5858585858585857\n",
      "seed :  5, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.6226415094339622\n",
      "stock :  ABIRLANUVO\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5572139303482587\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.4971751412429378\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.62882096069869\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.544502617801047\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6124401913875598\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5578947368421052\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.6146788990825689\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5803108808290155\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.5727699530516431\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.6176470588235294\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.45555555555555555\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.6574074074074074\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5108695652173912\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5922330097087377\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.6306306306306306\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5760869565217391\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.5686274509803921\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.6019417475728156\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5729729729729729\n",
      "seed :  6, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.6063348416289592\n",
      "stock :  ABIRLANUVO\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.49462365591397844\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.5728155339805825\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5507246376811594\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6326530612244898\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.57\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.5263157894736843\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5217391304347826\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.5603864734299517\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.6063348416289592\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.6130653266331658\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.5968586387434555\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.4632768361581921\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5027322404371585\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.5809523809523809\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5816326530612245\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.588785046728972\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.5588235294117647\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5759162303664922\n",
      "seed :  7, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.5728155339805825\n",
      "stock :  ABIRLANUVO\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5482233502538072\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5154639175257733\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.5990783410138248\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5268817204301076\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6392694063926941\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5797101449275363\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.6458333333333334\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.594059405940594\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.6572769953051644\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.5771144278606964\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.6458333333333334\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.6448598130841122\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5714285714285714\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5454545454545455\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.5951219512195123\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5806451612903225\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.6457399103139013\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.5728643216080402\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5463917525773196\n",
      "seed :  8, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.6454545454545454\n",
      "stock :  ABIRLANUVO\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.5405405405405405\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.4816753926701571\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.6291079812206574\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5741626794258373\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.6122448979591836\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.6066350710900476\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.6060606060606061\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5608465608465608\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.5846153846153845\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.59\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.5794392523364486\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.6176470588235294\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.6185567010309277\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5792349726775956\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  15\n",
      "F1_score :  0.6063348416289592\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.6513761467889908\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.59\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.5948717948717949\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5482233502538072\n",
      "seed :  9, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.6036036036036037\n",
      "stock :  ABIRLANUVO\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  1\n",
      "F1_score :  0.6019417475728156\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  2\n",
      "F1_score :  0.5396825396825398\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  3\n",
      "F1_score :  0.6161137440758293\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  4\n",
      "F1_score :  0.5425531914893618\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  5\n",
      "F1_score :  0.561576354679803\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  6\n",
      "F1_score :  0.5595854922279793\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  7\n",
      "F1_score :  0.5686274509803921\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  8\n",
      "F1_score :  0.5177664974619289\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  9\n",
      "F1_score :  0.5970149253731344\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  10\n",
      "F1_score :  0.561576354679803\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  11\n",
      "F1_score :  0.5654450261780105\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  12\n",
      "F1_score :  0.6009852216748768\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  13\n",
      "F1_score :  0.5583756345177665\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  14\n",
      "F1_score :  0.5226130653266332\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_score :  0.6153846153846153\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  16\n",
      "F1_score :  0.5154639175257733\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  17\n",
      "F1_score :  0.6325581395348837\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  18\n",
      "F1_score :  0.5631067961165048\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  19\n",
      "F1_score :  0.5876288659793815\n",
      "seed :  10, num_clfs :  5, depth :  4, random_state :  20\n",
      "F1_score :  0.5933014354066986\n",
      "rf_test_acc_dict data:\n",
      "        index  seed  depth  num_clfs  random_state  F1_score  Precision  \\\n",
      "0  ABIRLANUVO     1      4         5             1  0.660099      0.583   \n",
      "\n",
      "   Recall    AUC    AR  found  \n",
      "0   0.761  0.572 -0.41      1  \n",
      "final_results_df data:\n",
      "     ytrue  ypred         0         1      SYMBOL\n",
      "0  1390.35      0  0.524921  0.475079  ABIRLANUVO\n",
      "1  1387.30      0  0.514610  0.485390  ABIRLANUVO\n",
      "time taken for one stock grid-search tuning with RF:  0:00:06.342249\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooja/Desktop/paper/DeConFuse/codes/data_processing.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by = ['DATE'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock :  ACC\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Reg2/TL_Train/ACC_op1_4_op2_8_mp1_True_mp2_False_ks1_5_ks2_3_new_0.2_tl_xtrain1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a3b900f96dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mxte_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'data/Reg2/TL_Test/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstock\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam_path\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_tl_xtest'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0myte_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'data/Reg2/TL_Test/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstock\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam_path\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_tl_ytest'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mZtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mZtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxte_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Reg2/TL_Train/ACC_op1_4_op2_8_mp1_True_mp2_False_ks1_5_ks2_3_new_0.2_tl_xtrain1.npy'"
     ]
    }
   ],
   "source": [
    "gap = 5.0\n",
    "log_interval = 1\n",
    "tr_acc_min = 54\n",
    "cnt = 0 \n",
    "pos_label = 1\n",
    "depth = 4\n",
    "num_clfs = 5\n",
    "rf_test_acc_dict  = {}\n",
    "final_results_df = pd.DataFrame()\n",
    "t0 = time.time()\n",
    "for stock in stocks_list[start:end]:\n",
    "    t01 = time.time()\n",
    "    temp_dict = {}\n",
    "    rf_test_acc_dict[stock] = {}\n",
    "    best_tr_acc = 0.0\n",
    "    best_te_acc = 0.0\n",
    "    best_f1_score = 0.0\n",
    "    f1_score = 0.0\n",
    "    best_ar = 0.0\n",
    "    found = 0\n",
    "    ytr_pred, yte_pred, tr_scores, te_score = [],[],[],[]\n",
    "    AR = 0\n",
    "    temp_ytr_pred, temp_yte_pred, temp_tr_scores, temp_te_scores = [],[],[],[]\n",
    "    temp_f1_score = 0\n",
    "    temp_auc = 0\n",
    "    temp_AR= 0\n",
    "    _,windowed_data,_, _ = getWindowedDataReg(data_df,stock,window_size)\n",
    "    feat_wise_data = getFeatWiseData(windowed_data,features_list)\n",
    "    prev_day_values = getPrevDayFeatures(feat_wise_data)\n",
    "    prev_day_values = prev_day_values[:,0]\n",
    "    for sd in range(1,seed_range+1):\n",
    "        seed = sd\n",
    "        print('stock : ', stock)\n",
    "        xtr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +'_' + str(test_size) + '_tl_xtrain' + str(seed) + '.npy'\n",
    "        ytr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +  '_' + str(test_size) + '_tl_ytrain' + str(seed) + '.npy'\n",
    "        xte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_xtest' + str(seed) + '.npy'\n",
    "        yte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_ytest' + str(seed) + '.npy'\n",
    "        Ztrain = np.load(xtr_path)\n",
    "        Y_train = np.load(ytr_path)\n",
    "        Ztest = np.load(xte_path)\n",
    "        Y_test = np.load(yte_path)\n",
    "        ytr_prev_day = prev_day_values[:Y_train.shape[0]]\n",
    "        yte_prev_day  = prev_day_values[Y_train.shape[0]:]\n",
    "        yte_prev_day  = yte_prev_day[:yte_prev_day.shape[0]-1]\n",
    "\n",
    "        for random_state in range(1,21):\n",
    "            print('seed : {: }, num_clfs : {: }, depth : {: }, random_state : {: }'.format(seed,num_clfs,depth,random_state))\n",
    "            Y_train_true_labels = np.where((Y_train - ytr_prev_day)>0,1,0)\n",
    "            Y_test_true_labels = np.where((Y_test - yte_prev_day)>0,1,0)\n",
    "            ytr_pred, yte_pred, tr_scores, te_scores = clfRF(Ztrain,Y_train_true_labels, Ztest, Y_test_true_labels, n_clf=num_clfs,\n",
    "                                                                        depth=depth, rnd_state=random_state)\n",
    "            limit = Ztrain.shape[0]\n",
    "            precision, recall, f1_score,_ = precision_recall_fscore_support(Y_test_true_labels, yte_pred, pos_label=1, average='binary')\n",
    "            \n",
    "            print('F1_score : ',f1_score)\n",
    "\n",
    "            if f1_score > best_f1_score:\n",
    "                found = 1\n",
    "                AR = compAnnualReturns(stock,yte_pred,data_df,window_size,limit)\n",
    "                print('AR: ',AR)\n",
    "                fpr, tpr, thresholds = roc_curve(Y_test_true_labels, te_scores[:,pos_label], pos_label = pos_label)\n",
    "                AUC_val = auc(fpr, tpr)\n",
    "                best_ar = AR\n",
    "                best_te_acc = te_acc\n",
    "                best_f1_score = f1_score\n",
    "                rf_test_acc_dict[stock]['seed'] = seed\n",
    "                rf_test_acc_dict[stock]['depth'] = depth\n",
    "                rf_test_acc_dict[stock]['num_clfs'] = num_clfs\n",
    "                rf_test_acc_dict[stock]['random_state'] = random_state\n",
    "                rf_test_acc_dict[stock]['F1_score'] = round(f1_score,3)\n",
    "                rf_test_acc_dict[stock]['Precision'] = round(precision,3)\n",
    "                rf_test_acc_dict[stock]['Recall'] = round(recall,3)\n",
    "                rf_test_acc_dict[stock]['AUC'] = round(AUC_val,3)\n",
    "                rf_test_acc_dict[stock]['AR'] = best_ar\n",
    "                rf_test_acc_dict[stock]['found'] = found\n",
    "                temp_dict['yte_pred'] = yte_pred\n",
    "                temp_dict['te_scores'] = te_scores\n",
    "            \n",
    "    temp_final_df = pd.DataFrame(Y_test,columns=['ytrue'])\n",
    "    temp_final_df['ypred'] = temp_dict['yte_pred']\n",
    "    temp_scores_df = pd.DataFrame(temp_dict['te_scores']) \n",
    "    temp_final_df = pd.concat([temp_final_df,temp_scores_df],axis = 1)\n",
    "    temp_final_df['SYMBOL'] = stock\n",
    "    final_results_df = pd.concat([final_results_df,temp_final_df],axis = 0)\n",
    "    cnt += 1\n",
    "    if cnt%log_interval==0:\n",
    "        df = pd.DataFrame.from_dict(data = rf_test_acc_dict, orient = 'index').reset_index()\n",
    "        print('rf_test_acc_dict data:')\n",
    "        print(df.head(2))\n",
    "        if not os.path.exists(rf_res_file_name):\n",
    "            df.to_csv(rf_res_file_name,index=None, header='column_names')\n",
    "        else: # else it exists so append without writing the header\n",
    "            df.to_csv(rf_res_file_name, mode='a',index=None, header=False)\n",
    "        if not os.path.exists(rf_pred_file_name):\n",
    "            final_results_df.to_csv(rf_pred_file_name,index=None, header='column_names')\n",
    "        else: # else it exists so append without writing the header\n",
    "            final_results_df.to_csv(rf_pred_file_name, mode='a',index=None, header=False)\n",
    "        print('final_results_df data:')\n",
    "        print(final_results_df.head(2))\n",
    "        rf_test_acc_dict = {}\n",
    "        final_results_df = pd.DataFrame()\n",
    "    t11 = time.time()\n",
    "    print('time taken for one stock grid-search tuning with RF: ' ,datetime.timedelta(seconds = t11 - t01))\n",
    "    print('*'*100)\n",
    "t1 = time.time()\n",
    "print('time taken for all stocks grid-search tuning with RF: ' ,datetime.timedelta(seconds = t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict other Regression Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: 0\n",
      "stock: ABIRLANUVO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyoti\\Desktop\\Pooja\\Stock_Trend_Classification\\codes\\data_processing.py:117: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  X.sort_values(by = ['DATE'],inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.787045\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ACC\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.047060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ADANIENT\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.007057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ADANIPORTS\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.988056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ADANIPOWER\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.996057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: AJANTPHARM\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.730042\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ALBK\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.026059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: AMARAJABAT\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.724042\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: AMBUJACEM\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.065061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ANDHRABANK\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:01.004057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: APOLLOHOSP\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.027059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: APOLLOTYRE\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.041059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ARVIND\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.035059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ASHOKLEY\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.996057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ASIANPAINT\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.013058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: AUROPHARMA\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.014058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: AXISBANK\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.064061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BAJAJ-AUTO\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.006058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BAJFINANCE\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:00.761044\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BANKBARODA\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:01.051060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BANKINDIA\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.035059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BATAINDIA\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.011058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BEL\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.772044\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BHARATFORG\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:01.058060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BHARTIARTL\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.071061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BHEL\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.066061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BIOCON\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.034059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BOSCHLTD\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.876050\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BPCL\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.053060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: BRITANNIA\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.791045\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CAIRN\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.793045\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CANBK\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.053060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CASTROLIND\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:00.781045\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CEATLTD\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.786045\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CENTURYTEX\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.053060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CESC\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.062061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: CIPLA\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.050060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: COALINDIA\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:01.028059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: COLPAL\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.046060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: DABUR\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.075062\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: DHFL\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.772044\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: DISHTV\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:01.078062\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: DIVISLAB\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.038059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: DLF\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.027059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: DRREDDY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.040059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: EICHERMOT\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:00.912052\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ENGINERSIN\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.855049\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: EXIDEIND\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.042059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: FEDERALBNK\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.067061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: GAIL\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.062060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: GLENMARK\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.013058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: GMRINFRA\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.992057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: GODREJIND\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.035059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: GRASIM\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.035059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HAVELLS\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.059060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HCLTECH\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.022058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HDFC\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.002057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HDFCBANK\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:00.999058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HDIL\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.917052\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HEROMOTOCO\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.996057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HEXAWARE\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:00.989057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HINDALCO\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.986057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HINDPETRO\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.012058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HINDUNILVR\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:01.075062\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: HINDZINC\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.054060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IBREALEST\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:00.892051\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IBULHSGFIN\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.877050\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ICICIBANK\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.077062\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IDBI\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.060061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IDEA\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.049060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IDFC\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.076062\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IFCI\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.056060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IGL\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:01.090062\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: INDIACEM\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.063061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: INDUSINDBK\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.045060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: INFY\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.059060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IOC\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.056061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: IRB\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.052060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ITC\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:01.046060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JINDALSTEL\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.071061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JISLJALEQS\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.031059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JPASSOCIAT\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.047060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JSWENERGY\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:00.923053\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JSWSTEEL\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.037059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JUBLFOOD\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.046060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: JUSTDIAL\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.998057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: KOTAKBANK\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:01.035059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: KSCL\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.778045\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: KTKBANK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.056060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: L&TFH\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.995057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: LICHSGFIN\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.062061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: LT\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.034059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: LUPIN\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.054060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: M&M\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.064061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: M&MFIN\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.037059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: MARUTI\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.047060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: MINDTREE\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:00.918052\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: MOTHERSUMI\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.895051\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: MRF\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:01.045060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: NHPC\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.029059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: NMDC\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.029059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: NTPC\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.030059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: OFSS\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.029059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: OIL\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.768044\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ONGC\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:01.072061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ORIENTBANK\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.019058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: PAGEIND\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.760043\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: PETRONET\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.030059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: PFC\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:01.061061\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: PIDILITIND\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.774044\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: PNB\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:01.044060\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: POWERGRID\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.985056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: PTC\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:01.008058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: RCOM\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.987056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: RECLTD\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.996057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: RELCAPITAL\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.979056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: RELIANCE\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.986057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: RELINFRA\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.982056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: RPOWER\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.997057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SAIL\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.985056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SBIN\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:01.034059\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SIEMENS\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:01.010058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SOUTHBANK\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.730042\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SRF\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:00.728042\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SRTRANSFIN\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.977056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: STAR\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:00.802046\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SUNPHARMA\n",
      "seed :  4\n",
      "time taken for one stock with ridge:  0:00:00.988056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SUNTV\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.985056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: SYNDIBANK\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:00.980056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATACHEM\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:00.979056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATACOMM\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:00.988057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATAGLOBAL\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.983056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATAMOTORS\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.978056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATAMTRDVR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.995057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATAPOWER\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.977056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TATASTEEL\n",
      "seed :  3\n",
      "time taken for one stock with ridge:  0:00:00.969055\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TCS\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.987056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TECHM\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:01.011058\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TITAN\n",
      "seed :  1\n",
      "time taken for one stock with ridge:  0:00:00.990057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: TVSMOTOR\n",
      "seed :  9\n",
      "time taken for one stock with ridge:  0:00:00.851049\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: UBL\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.971056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ULTRACEMCO\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.989057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: UNIONBANK\n",
      "seed :  8\n",
      "time taken for one stock with ridge:  0:00:00.988057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: UPL\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:01.002057\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: VEDL\n",
      "seed :  2\n",
      "time taken for one stock with ridge:  0:00:00.747043\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: VOLTAS\n",
      "seed :  7\n",
      "time taken for one stock with ridge:  0:00:00.975056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: WIPRO\n",
      "seed :  10\n",
      "time taken for one stock with ridge:  0:00:00.981056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: WOCKPHARMA\n",
      "seed :  5\n",
      "time taken for one stock with ridge:  0:00:00.816047\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: YESBANK\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.986056\n",
      "****************************************************************************************************\n",
      "cnt: 0\n",
      "stock: ZEEL\n",
      "seed :  6\n",
      "time taken for one stock with ridge:  0:00:00.984056\n",
      "****************************************************************************************************\n",
      "time taken for all stocks with ridge:  0:02:27.634444\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "random_state = 1\n",
    "test_pred_dict = {}\n",
    "test_measures_dict = {}\n",
    "cnt = 0\n",
    "t0 = time.time()\n",
    "for stock in stocks_list[start:end]: \n",
    "    t01 = time.time()\n",
    "    print('cnt:',cnt)\n",
    "    print('stock:',stock)\n",
    "    test_pred_dict[stock] = {}\n",
    "    test_measures_dict[stock] = {}\n",
    "    _, _, _, next_day_values = getWindowedDataReg(data_df,stock,window_size)\n",
    "    next_day_values = next_day_values[0:next_day_values.shape[0]-1]\n",
    "    seed = int(layers2_res_df.loc[layers2_res_df['index']==stock]['seed'].values.tolist()[0])\n",
    "    print('seed : ',seed)\n",
    "    xtr_path = base_path + 'data/Reg2/TL_Train/' + stock + param_path +'_' + str(test_size) + '_tl_xtrain' + str(seed) + '.npy'\n",
    "    xte_path = base_path + 'data/Reg2/TL_Test/' + stock + param_path +  '_' + str(test_size) + '_tl_xtest' + str(seed) + '.npy'\n",
    "    Ztrain = np.load(xtr_path)\n",
    "    Ztest = np.load(xte_path)\n",
    "    #for i in range(1,5):\n",
    "    limit = Ztrain.shape[0]\n",
    "    next_open_prices, next_high_prices, next_low_prices, next_day_volume = next_day_values[:,1],next_day_values[:,2],\\\n",
    "                                                                        next_day_values[:,3], next_day_values[:,4]\n",
    "    Y_train_op = next_open_prices[0:limit] \n",
    "    Y_test_op = next_open_prices[limit:]\n",
    "    y_pred_op, mae_op, mse_op, rmse_op = ridge_regressor(Ztrain, Y_train_op, Ztest, Y_test_op, alpha = alpha, random_state = random_state)\n",
    "    mae2_op = np.sum(np.abs(y_pred_op - Y_test_op))/np.sum(Y_test_op)\n",
    "    test_pred_dict[stock]['True_OP'] = Y_test_op\n",
    "    test_pred_dict[stock]['Predicted_OP'] = y_pred_op\n",
    "    test_measures_dict[stock]['MAE_OP'] = mae_op\n",
    "    test_measures_dict[stock]['MAE2_OP'] = mae2_op\n",
    "    test_measures_dict[stock]['MSE_OP'] = mse_op\n",
    "    test_measures_dict[stock]['RMSE_OP'] = rmse_op\n",
    "    \n",
    "    \n",
    "    Y_train_h = next_high_prices[0:limit] \n",
    "    Y_test_h = next_high_prices[limit:]\n",
    "    y_pred_h, mae_h, mse_h, rmse_h = ridge_regressor(Ztrain, Y_train_h, Ztest, Y_test_h, alpha = alpha, random_state = random_state)\n",
    "    mae2_h = np.sum(np.abs(y_pred_h - Y_test_h))/np.sum(Y_test_h)\n",
    "    test_pred_dict[stock]['True_HP'] = Y_test_h\n",
    "    test_pred_dict[stock]['Predicted_HP'] = y_pred_h\n",
    "    test_measures_dict[stock]['MAE_HP'] = mae_h\n",
    "    test_measures_dict[stock]['MAE2_HP'] = mae2_h\n",
    "    test_measures_dict[stock]['MSE_HP'] = mse_h\n",
    "    test_measures_dict[stock]['RMSE_HP'] = rmse_h\n",
    "    \n",
    "    Y_train_l = next_low_prices[0:limit] \n",
    "    Y_test_l = next_low_prices[limit:]\n",
    "    y_pred_l, mae_l, mse_l, rmse_l = ridge_regressor(Ztrain, Y_train_l, Ztest, Y_test_l, alpha = alpha, random_state = random_state)\n",
    "    mae2_l = np.sum(np.abs(y_pred_l - Y_test_l))/np.sum(Y_test_l)\n",
    "    test_pred_dict[stock]['True_LP'] = Y_test_l\n",
    "    test_pred_dict[stock]['Predicted_LP'] = y_pred_l\n",
    "    test_measures_dict[stock]['MAE_LP'] = mae_l\n",
    "    test_measures_dict[stock]['MAE2_LP'] = mae2_l\n",
    "    test_measures_dict[stock]['MSE_LP'] = mse_l\n",
    "    test_measures_dict[stock]['RMSE_LP'] = rmse_l\n",
    "    \n",
    "    Y_train_vol = next_day_volume[0:limit] \n",
    "    Y_test_vol = next_day_volume[limit:]\n",
    "    y_pred_vol, mae_vol, mse_vol, rmse_vol = ridge_regressor(Ztrain, Y_train_vol, Ztest, Y_test_vol, alpha = alpha, random_state = random_state)\n",
    "    mae2_vol = np.sum(np.abs(y_pred_vol - Y_test_vol))/np.sum(Y_test_vol)\n",
    "    test_pred_dict[stock]['True_Vol'] = Y_test_vol\n",
    "    test_pred_dict[stock]['Predicted_Vol'] = y_pred_vol\n",
    "    test_measures_dict[stock]['MAE_Vol'] = mae_vol\n",
    "    test_measures_dict[stock]['MAE2_Vol'] = mae2_vol\n",
    "    test_measures_dict[stock]['MSE_Vol'] = mse_vol\n",
    "    test_measures_dict[stock]['RMSE_Vol'] = rmse_vol\n",
    "    \n",
    "    t11 = time.time()\n",
    "    print('time taken for one stock with ridge: ' ,datetime.timedelta(seconds = t11 - t01))\n",
    "    print('*'*100)\n",
    "t1 = time.time()\n",
    "print('time taken for all stocks with ridge: ' ,datetime.timedelta(seconds = t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_file_name = base_path+'Results2/Reg2/res_reg2_' + param_path + 'final.csv'\n",
    "pred_file_name = base_path+'Results2/Reg2/res_reg2_' + param_path + '_pred_global_final.csv'\n",
    "if os.path.exists(res_file_name):\n",
    "    os.remove(res_file_name)\n",
    "if os.path.exists(pred_file_name):\n",
    "    os.remove(pred_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df = pd.DataFrame.from_dict(data = test_measures_dict, orient = 'index').reset_index()\n",
    "test_pred_df = pd.DataFrame.from_dict(data = test_pred_dict, orient = 'index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_df.to_csv(res_file_name,index=None, header='column_names')\n",
    "test_pred_df.to_csv(pred_file_name,index=None, header='column_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
